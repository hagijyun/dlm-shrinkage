% This file was created with JabRef 2.9b2.
% Encoding: ISO8859_1

@ARTICLE{ArmaganDunsonLee2011,
  author = {{Armagan}, A. and {Dunson}, D. and {Lee}, J.},
  title = {Generalized double Pareto shrinkage},
  year = {2011},
  month = apr,
  eprint = {1104.0861},
  adsnote = {Provided by the SAO/NASA Astrophysics Data System},
  adsurl = {http://adsabs.harvard.edu/abs/2011arXiv1104.0861A},
  archiveprefix = {arXiv},
  journal = {ArXiv e-prints},
  keywords = {Statistics - Methodology, Mathematics - Statistics Theory, Statistics
	- Machine Learning},
  owner = {jrnold},
  primaryclass = {stat.ME},
  timestamp = {2013.04.24}
}

@ARTICLE{BaeMallick2004,
  author = {Bae, Kyounghwa and Mallick, Bani K.},
  title = {Gene selection using a two-level hierarchical Bayesian model},
  year = {2004},
  volume = {20},
  number = {18},
  pages = {3423-3430},
  doi = {10.1093/bioinformatics/bth419},
  eprint = {http://bioinformatics.oxfordjournals.org/content/20/18/3423.full.pdf+html},
  url = {http://bioinformatics.oxfordjournals.org/content/20/18/3423.abstract},
  abstract = {Summary: The fundamental problem of gene selection via cDNA data is
	to identify which genes are differentially expressed across different
	kinds of tissue samples (e.g. normal and cancer). cDNA data contain
	large number of variables (genes) and usually the sample size is
	relatively small so the selection process can be unstable. Therefore,
	models which incorporate sparsity in terms of variables (genes) are
	desirable for this kind of problem. This paper proposes a two-level
	hierarchical Bayesian model for variable selection which assumes
	a prior that favors sparseness. We adopt a Markov chain Monte Carlo
	(MCMC) based computation technique to simulate the parameters from
	the posteriors. The method is applied to leukemia data from a previous
	study and a published dataset on breast cancer.Supplementary information:
	http://stat.tamu.edu/people/faculty/bmallick.html},
  journal = {Bioinformatics},
  owner = {jrnold}
}

@ARTICLE{Berger1980,
  author = {Berger, James},
  title = {A Robust Generalized Bayes Estimator and Confidence Region for a
	Multivariate Normal Mean},
  year = {1980},
  language = {English},
  volume = {8},
  number = {4},
  pages = {pp. 716-761},
  issn = {00905364},
  url = {http://www.jstor.org/stable/2240763},
  abstract = {It is observed that in selecting an alternative to the usual maximum
	likelihood estimator, ?0, of a multivariate normal mean, it is important
	to take into account prior information. Prior information in the
	form of a prior mean and a prior covariance matrix is considered.
	A generalized Bayes estimator is developed which is significantly
	better than ?0 if this prior information is correct and yet is very
	robust with respect to misspecification of the prior information.
	An associated confidence region is also constructed, and is shown
	to have very attractive size and probability of coverage.},
  copyright = {Copyright © 1980 Institute of Mathematical Statistics},
  journal = {The Annals of Statistics},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Jul., 1980},
  owner = {jrnold},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2013.04.24}
}

@UNPUBLISHED{Blackwell2012,
  author = {Matthew Blackwell},
  title = {Game-changers: Detecting shifts in the Flow of campaign contributions},
  year = {2012},
  date = {2012-10-23},
  url = {http://www.mattblackwell.org/files/papers/gamechangers.pdf},
  abstract = {In this paper, I introduce a Bayesian model for detecting changepoints
	in a time-series of contributions to candidates over the course of
	a campaign. this game-changers model is ideal for campaign contributions
	data because it allows for overdispersion, a key feature of contributions
	data. Furthermore, while many extant changepoint models force researchers
	to choose the number of changepoint ex ante, the game-changers model
	incorporates a Dirichlet process prior in order to estimate the number
	of changepoints along with their location. I demonstrate the usefulness
	of the model in data from the the Republican primary and the U.S.
	Senate elections.},
  owner = {jeff},
  quality = {1},
  timestamp = {2013.05.03}
}

@REPORT{BrownGriffin2005,
  author = {Philip J. Brown and Jim E. Griffin},
  title = {Alternative Prior Distributions for Variable Selection with Very
	Many More Variables than Observations},
  type = {Technical Report},
  institution = {University of Warwick},
  year = {2005},
  abstract = {The problem of variable selection in regression and the generalised
	linear model is addressed. We adopt a Bayesian approach with priors
	for the regression coefficients that are scale mixtures of normal
	distributions and embody a high prior probability of proximity to
	zero. By seeking modal estimates we generalise the lasso. Properties
	of the priors and their resultant posteriors are explored in the
	context of the linear and generalised linear model especially when
	there are more variables than observations. We develop EM algorithms
	that embrace the need to explore the multiple modes of the non log-concave
	posterior distributions. Finally we apply the technique to microarray
	data using a probit model to find the genetic predictors of osteo-
	versus rheumatoid arthritis.},
  keywords = {Bayesian modal analysis, Variable selection in regression, Scale mixtures
	of normals, Improper Jeffreys prior, lasso, Penalised likelihood,
	EMalgorithm, Multiple modes, More variables than observations, Singular
	value decomposition, Latent variables, Probit regression.},
  owner = {jrnold},
  timestamp = {2013.04.24},
  url = {http://wrap.warwick.ac.uk/35585/}
}

@INPROCEEDINGS{CaronDoucet2008,
  author = {Caron, Fran\c{c}ois and Doucet, Arnaud},
  title = {Sparse Bayesian nonparametric regression},
  booktitle = {Proceedings of the 25th international conference on Machine learning},
  year = {2008},
  series = {ICML '08},
  publisher = {ACM},
  location = {Helsinki, Finland},
  isbn = {978-1-60558-205-4},
  pages = {88--95},
  doi = {10.1145/1390156.1390168},
  url = {http://doi.acm.org/10.1145/1390156.1390168},
  abstract = {One of the most common problems in machine learning and statistics
	consists of estimating the mean response X? from a vector of observations
	y assuming y = X? + ? where X is known, ? is a vector of parameters
	of interest and ? a vector of stochastic errors. We are particularly
	interested here in the case where the dimension K of ? is much higher
	than the dimension of y. We propose some flexible Bayesian models
	which can yield sparse estimates of ?. We show that as K ? ? these
	models are closely related to a class of Lévy processes. Simulations
	demonstrate that our models outperform significantly a range of popular
	alternatives.},
  acmid = {1390168},
  numpages = {8},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@ARTICLE{CarvalhoPolsonScott2010,
  author = {Carvalho, Carlos M. and Polson, Nicholas G. and Scott, James G.},
  title = {The horseshoe estimator for sparse signals},
  year = {2010},
  volume = {97},
  number = {2},
  pages = {465-480},
  doi = {10.1093/biomet/asq017},
  eprint = {http://biomet.oxfordjournals.org/content/97/2/465.full.pdf+html},
  url = {http://biomet.oxfordjournals.org/content/97/2/465.abstract},
  abstract = {This paper proposes a new approach to sparsity, called the horseshoe
	estimator, which arises from a prior based on multivariate-normal
	scale mixtures. We describe the estimator?s advantages over existing
	approaches, including its robustness, adaptivity to different sparsity
	patterns and analytical tractability. We prove two theorems: one
	that characterizes the horseshoe estimator?s tail robustness and
	the other that demonstrates a super-efficient rate of convergence
	to the correct estimate of the sampling density in sparse situations.
	Finally, using both real and simulated data, we show that the horseshoe
	estimator corresponds quite closely to the answers obtained by Bayesian
	model averaging under a point-mass mixture prior.},
  journal = {Biometrika},
  owner = {jrnold},
  timestamp = {2013.03.01}
}

@ARTICLE{CarvalhoPolsonScott2009,
  author = {Carlos M. Carvalho and Nicholas G. Polson and James G. Scott},
  title = {Handling Sparsity via the Horseshoe},
  journaltitle = {Journal of Machine Learning and Research: Workshop and Conference
	Proceedings},
  year = {2009},
  volume = {5},
  pages = {73-80},
  abstract = {This paper presents a general, fully Bayesian framework for sparse
	supervised-learning problems based on the horseshoe prior. The horseshoe
	prior is a member of the family of multivariate scale mixtures of
	normals, and is therefore closely related to widely used approaches
	for sparse Bayesian learning, including, among others, Laplacian
	priors (e.g. the LASSO) and Student-t priors (e.g. the relevance
	vector machine). The advantages of the horseshoe are its robustness
	at handling unknown sparsity and large outlying signals. These properties
	are justifed theoretically via a representation theorem and accompanied
	by comprehensive empirical experiments that compare its performance
	to benchmark alternatives.},
  owner = {jrnold},
  timestamp = {2013.03.01}
}

@ARTICLE{Chib1998,
  author = {Siddhartha Chib},
  title = {Estimation and comparison of multiple change-point models},
  year = {1998},
  volume = {86},
  number = {2},
  pages = {221 - 241},
  issn = {0304-4076},
  doi = {DOI: 10.1016/S0304-4076(97)00115-2},
  url = {http://www.sciencedirect.com/science/article/B6VC0-3VM1XM5-2/2/469ee3cba827365611dee3677f0babc6},
  abstract = { This paper provides a new Bayesian approach for models with multiple
	change points. The centerpiece of the approach is a formulation of
	the change-point model in terms of a latent discrete state variable
	that indicates the regime from which a particular observation has
	been drawn. This state variable is specified to evolve according
	to a discrete-time discrete-state Markov process with the transition
	probabilities constrained so that the state variable can either stay
	at the current value or jump to the next higher value. This parameterization
	exactly reproduces the change point model. The model is estimated
	by Markov chain Monte Carlo methods using an approach that is based
	on Chib (1996). This methodology is quite valuable since it allows
	for the fitting of more complex change point models than was possible
	before. Methods for the computation of Bayes factors are also developed.
	All the techniques are illustrated using simulated and real data
	sets.},
  journal = {Journal of Econometrics},
  owner = {jrnold}
}

@ARTICLE{Cobb1978,
  author = {Cobb, George W.},
  title = {The problem of the Nile: Conditional solution to a changepoint problem},
  year = {1978},
  volume = {65},
  number = {2},
  pages = {243-251},
  doi = {10.1093/biomet/65.2.243},
  eprint = {http://biomet.oxfordjournals.org/content/65/2/243.full.pdf+html},
  url = {http://biomet.oxfordjournals.org/content/65/2/243.abstract},
  abstract = {Inference is considered for the point in a sequence of random variables
	at which the probability distribution changes. An approximation to
	the conditional distribution of the maximum likelihood estimator
	of the changepoint given the ancillary values of observations adjacent
	to the estimated changepoint is derived and shown to be numerically
	equal to a Bayesian posterior distribution for the changepoint. A
	hydrological example is given to show that inferences based on the
	conditional distribution of the maximum likelihood estimator can
	differ sharply from inferences based on the marginal distribution},
  journal = {Biometrika},
  owner = {jeff},
  timestamp = {2013.04.22}
}

@BOOK{CommandeurKoopman2007,
  author = {Commandeur, Jacques J.F. and Koopman, Siem Jan},
  title = {An Introduction to State Space Time Series Analysis},
  year = {2007},
  series = {Practical Econometrics Series},
  publisher = {OUP Oxford},
  isbn = {9780191607806},
  url = {http://books.google.com/books?id=OCmljPYfgkUC},
  file = {CommandeurKoopman2007.pdf:CommandeurKoopman2007.pdf:PDF},
  owner = {jrnold}
}

@ARTICLE{CommandeurKoopmanOoms2011,
  author = {Jacques J. F. Commandeur and Siem Jan Koopman and Marius Ooms},
  title = {Statistical Software for State Space Methods},
  year = {2011},
  volume = {41},
  number = {1},
  month = {5},
  pages = {1--18},
  issn = {1548-7660},
  url = {http://www.jstatsoft.org/v41/i01},
  abstract = {In this paper we review the state space approach to time series analysis
	and establish the notation that is adopted in this special volume
	of the Journal of Statistical Software. We first provide some background
	on the history of state space methods for the analysis of time series.
	This is followed by a concise overview of linear Gaussian state space
	analysis including the modelling framework and appropriate estimation
	methods. We discuss the important class of unobserved component models
	which incorporate a trend, a seasonal, a cycle, and fixed explanatory
	and intervention variables for the univariate and multivariate analysis
	of time series. We continue the discussion by presenting methods
	for the computation of different estimates for the unobserved state
	vector: filtering, prediction, and smoothing. Estimation approaches
	for the other parameters in the model are also considered. Next,
	we discuss how the estimation procedures can be used for constructing
	confidence intervals, detecting outlier observations and structural
	breaks, and testing model assumptions of residual independence, homoscedasticity,
	and normality. We then show how ARIMA and ARIMA components models
	fit in the state space framework to time series analysis. We also
	provide a basic introduction for non-Gaussian state space models.
	Finally, we present an overview of the software tools currently available
	for the analysis of time series with state space methods as they
	are discussed in the other contributions to this special volume.},
  accepted = {2010-12-20},
  bibdate = {2010-12-20},
  coden = {JSSOBK},
  day = {12},
  journal = {Journal of Statistical Software},
  owner = {jrnold},
  submitted = {2009-08-22},
  timestamp = {2013.01.21}
}

@BOOK{DurbinKoopman2001,
  author = {Durbin, J. and Koopman, S.S.J.},
  title = {Time Series Analysis by State Space Methods},
  year = {2001},
  series = {Oxford statistical sciences series},
  publisher = {Oxford University Press, Incorporated},
  isbn = {9780198523543},
  url = {http://books.google.com/books?id=XRCu5iSz\_HwC},
  lccn = {00054845},
  owner = {jeff},
  timestamp = {2013.04.22}
}

@ARTICLE{FigueiredoMember2003,
  author = {Mario A.T. Figueiredo and Senior Member},
  title = {Adaptive Sparseness for Supervised Learning},
  year = {2003},
  volume = {25},
  pages = {1150--1159},
  abstract = {The goal of supervised learning is to infer a functional mapping based
	on a set of training examples. To achieve good generalization, it
	is necessary to control the "complexity" of the learned function.
	In Bayesian approaches, this is done by adopting a prior for the
	parameters of the function being learned. We propose a Bayesian approach
	to supervised learning, which leads to sparse solutions; that is,
	in which irrelevant parameters are automatically set exactly to zero.
	Other ways to obtain sparse classifiers (such as Laplacian priors,
	support vector machines) involve (hyper)parameters which control
	the degree of sparseness of the resulting classifiers; these parameters
	have to be somehow adjusted/estimated from the training data. In
	contrast, our approach does not involve any (hyper)parameters to
	be adjusted or estimated. This is achieved by a hierarchical-Bayes
	interpretation of the Laplacian prior, which is then modified by
	the adoption of a Jeffreys' noninformative hyperprior. Implementation
	is carried out by an expectationmaximization (EM) algorithm. Experiments
	with several benchmark data sets show that the proposed approach
	yields state-of-the-art performance. In particular, our method outperforms
	SVMs and performs competitively with the best alternative techniques,
	although it involves no tuning or adjustment of sparseness-controlling
	hyperparameters.},
  journal = {IEEE Transactions on Pattern Analysis and Machine Intelligence},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@ARTICLE{Gelman2013,
  author = {Gelman, Andrew},
  title = {Commentary: P Values and Statistical Practice},
  year = {2013},
  volume = {24},
  number = {1},
  pages = {69--72
	
	10.1097/EDE.0b013e31827886f7},
  url = {http://journals.lww.com/epidem/Fulltext/2013/01000/Commentary___P_Values_and_Statistical_Practice.10.aspx},
  __markedentry = {[jeff:6]},
  journal = {Epidemiology},
  keywords = {00001648-201301000-00010},
  owner = {jeff},
  timestamp = {2013.05.03}
}

@ARTICLE{GiordaniKohn2008,
  author = {Giordani, Paolo and Kohn, Robert},
  title = {Efficient Bayesian Inference for Multiple Change-Point and Mixture
	Innovation Models},
  year = {2008},
  volume = {26},
  number = {1},
  pages = {66-77},
  doi = {10.1198/073500107000000241},
  eprint = {http://amstat.tandfonline.com/doi/pdf/10.1198/073500107000000241},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/073500107000000241},
  abstract = { Time series subject to parameter shifts of random magnitude and timing
	are commonly modeled with a change-point approach using Chib's algorithm
	to draw the break dates. We outline some advantages of an alternative
	approach in which breaks come through mixture distributions in state
	innovations, and for which the sampler of Gerlach, Carter, and Kohn
	allows reliable and efficient inference. We show how the same sampler
	can be used to model shifts in variance that occur independently
	of shifts in other parameters and how to draw the break dates efficiently
	when regime durations follow a Poisson process. Finally, we introduce
	to the time series literature the concept of adaptive MetropolisâHastings
	sampling for discrete latent variable models. We develop an easily
	implemented adaptive algorithm that improves on the work of Gerlach
	et al. and promises to significantly reduce computing time in a variety
	of problems including mixture innovation, change-point, regime switching,
	and outlier detection. The efficiency gains on two models for U.S.
	inflation and real interest rates are 257% and 341%. },
  journal = {Journal of Business \& Economic Statistics},
  owner = {jeff},
  timestamp = {2013.04.22}
}

@ARTICLE{Hans2009,
  author = {Hans, Chris},
  title = {Bayesian lasso regression},
  year = {2009},
  volume = {96},
  number = {4},
  pages = {835-845},
  doi = {10.1093/biomet/asp047},
  eprint = {http://biomet.oxfordjournals.org/content/96/4/835.full.pdf+html},
  url = {http://biomet.oxfordjournals.org/content/96/4/835.abstract},
  abstract = {The lasso estimate for linear regression corresponds to a posterior
	mode when independent, double-exponential prior distributions are
	placed on the regression coefficients. This paper introduces new
	aspects of the broader Bayesian treatment of lasso regression. A
	direct characterization of the regression coefficients? posterior
	distribution is provided, and computation and inference under this
	characterization is shown to be straightforward. Emphasis is placed
	on point estimation using the posterior mean, which facilitates prediction
	of future observations via the posterior predictive distribution.
	It is shown that the standard lasso prediction method does not necessarily
	agree with model-based, Bayesian predictions. A new Gibbs sampler
	for Bayesian lasso regression is introduced.},
  journal = {Biometrika},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@ARTICLE{HarveyKoopman2000,
  author = {Harvey, Andrew and Koopman, Siem Jan},
  title = {Signal extraction and the formulation of unobserved components models},
  year = {2000},
  volume = {3},
  number = {1},
  pages = {84--107},
  issn = {1368-423X},
  doi = {10.1111/1368-423X.00040},
  url = {http://dx.doi.org/10.1111/1368-423X.00040},
  abstract = {This paper looks at unobserved components models and examines the
	implied weighting patterns for signal extraction. There are four
	main themes. The first concerns the implications of correlated disturbances
	driving the components, especially those cases in which the correlation
	is perfect. The second is about the way in which ARIMA-based methods
	for trend extraction relate to those based on unobserved components.
	The third explores the impact of heteroscedasticity and irregular
	spacing and shows how setting up models with t-distributed disturbances
	leads to weighting patterns which are robust to outliers and breaks.
	Finally, a comparison is made between implied weighting patterns
	with kernels used in non-parametric trend estimation and equivalent
	kernels used in spline smoothing. It is demonstrated that with irregularly
	spaced data, the weighting used by conventional spline smoothing
	techniques is not the same as that obtained from the time series
	model based approach.},
  journal = {Econometrics Journal},
  keywords = {Cubic splines, Kalman filter and smoother, Kernels, Robustness, Structural
	time series model, Trend, WienerâKolmogorov filter.},
  owner = {jeff},
  publisher = {Blackwell Publishers Ltd},
  timestamp = {2013.04.22}
}

@ARTICLE{LiGoel2006,
  author = {Bin Li and Prem K. Goel},
  title = {Regularized Optimization in Statistical Learning: A Bayesian Perspective},
  journaltitle = {Statistica Sinica},
  year = {2006},
  number = {2},
  issue = {16},
  pages = {411-424},
  abstract = {Regularization plays a major role in modern data analysis, whenever
	non-regularized fitting is likely to lead to over-fitted model. It
	is known that most regularized optimization problems have Bayesian
	interpretation in which the prior plays the role of the regularizer.
	In this paper, we consider the issue of sensitivity of the regularized
	solution to the prior specification within the Bayesian perspective.
	We suggest a class of flat-tailed priors for a general likelihood
	function for robust Bayesian solutions, in the same spirit as the
	-distribution being suggested as a flat-tail prior for normal likelihood.
	Results are applied to a family of regularized learning methods and
	group LASSO. In addition, the consistency issue for LASSO is discussed
	within this framework.},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@ARTICLE{Park2011,
  author = {Park, Jong Hee},
  title = {Changepoint Analysis of Binary and Ordinal Probit Models: An Application
	to Bank Rate Policy Under the Interwar Gold Standard},
  year = {2011},
  doi = {10.1093/pan/mpr007},
  eprint = {http://pan.oxfordjournals.org/content/early/2011/03/22/pan.mpr007.full.pdf+html},
  url = {http://pan.oxfordjournals.org/content/early/2011/03/22/pan.mpr007.abstract},
  abstract = {In this paper, I introduce changepoint models for binary and ordered
	time series data based on Chib's hidden Markov model. The extension
	of the changepoint model to a binary probit model is straightforward
	in a Bayesian setting. However, detecting parameter breaks from ordered
	regression models is difficult because ordered time series data often
	have clustering along the break points. To address this issue, I
	propose an estimation method that uses the linear regression likelihood
	function for the sampling of hidden states of the ordinal probit
	changepoint model. The marginal likelihood method is used to detect
	the number of hidden regimes. I evaluate the performance of the introduced
	methods using simulated data and apply the ordinal probit changepoint
	model to the study of Eichengreen, Watson, and Grossman on violations
	of the ârules of the gameâ of the gold standard by the Bank of
	England during the interwar period.},
  journal = {Political Analysis},
  owner = {jrnold}
}

@ARTICLE{Park2010,
  author = {Park, Jong Hee},
  title = {Structural Change in U.S. Presidents' Use of Force},
  year = {2010},
  volume = {54},
  number = {3},
  pages = {766--782},
  issn = {1540-5907},
  doi = {10.1111/j.1540-5907.2010.00459.x},
  url = {http://dx.doi.org/10.1111/j.1540-5907.2010.00459.x},
  abstract = {Has there been a structural change in the way U.S. presidents use
	force abroad since the nineteenth century? In this article, I investigate
	historical changes in the use of force by U.S. presidents using Bayesian
	changepoint analysis. In doing so, I present an integrated Bayesian
	approach for analyzing changepoint problems in a Poisson regression
	model. To find the nature of the breaks, I estimate parameters of
	the Poisson regression changepoint model using Chib's (1998) hidden
	Markov model algorithm and FrÃ¼hwirth-Schnatter and Wagner's (2006)
	data augmentation method. Then, I utilize transdimensional Markov
	chain Monte Carlo methods to detect the number of breaks. Analyzing
	yearly use of force data from 1890 to 1995, I find that, controlling
	for the effects of the Great Depression and the two world wars, the
	relationship between domestic conditions and the frequency of the
	use of force abroad fundamentally shifted in the 1940s.},
  journal = {American Journal of Political Science},
  owner = {jrnold},
  publisher = {Blackwell Publishing Inc}
}

@ARTICLE{ParkCasella2008,
  author = {Park, Trevor and Casella, George},
  title = {The Bayesian Lasso},
  year = {2008},
  volume = {103},
  number = {482},
  pages = {681-686},
  doi = {10.1198/016214508000000337},
  eprint = {http://amstat.tandfonline.com/doi/pdf/10.1198/016214508000000337},
  url = {http://amstat.tandfonline.com/doi/abs/10.1198/016214508000000337},
  abstract = { The Lasso estimate for linear regression parameters can be interpreted
	as a Bayesian posterior mode estimate when the regression parameters
	have independent Laplace (i.e., double-exponential) priors. Gibbs
	sampling from this posterior is possible using an expanded hierarchy
	with conjugate normal priors for the regression parameters and independent
	exponential priors on their variances. A connection with the inverse-Gaussian
	distribution provides tractable full conditional distributions. The
	Bayesian Lasso provides interval estimates (Bayesian credible intervals)
	that can guide variable selection. Moreover, the structure of the
	hierarchical model provides both Bayesian and likelihood methods
	for selecting the Lasso parameter. Slight modifications lead to Bayesian
	versions of other Lasso-related estimation methods, including bridge
	regression and a robust variant. },
  journal = {Journal of the American Statistical Association},
  owner = {jrnold},
  timestamp = {2013.04.24}
}

@BOOK{PetrisPetroneEtAl2009,
  author = {Petris, G. and Petrone, S. and Campagnoli, P.},
  title = {{Dynamic Linear Models with R}},
  year = {2009},
  series = {Use R!},
  publisher = {Springer},
  isbn = {9780387772370},
  url = {http://books.google.com/books?id=VCt3zVq8TO8C},
  file = {PetrisPetroneEtAl2009.pdf:PetrisPetroneEtAl2009.pdf:PDF},
  lccn = {2009926480},
  owner = {jrnold},
  timestamp = {2010.11.10}
}

@ARTICLE{PolsonScott2010,
  author = {Nicholas G. Polson and James G. Scott},
  title = {Shrink Globally, Act Locally: Sparse Bayesian Regularization and
	Prediction},
  journaltitle = {Bayesian Statistics},
  year = {2010},
  abstract = {We use Levy processes to generate joint prior distributions for a
	location
	
	parameter ? = (?1 , . . . , ?p ) as p grows large. This approach,
	which generalizes
	
	normal scale-mixture priors to an infinite-dimensional setting, has
	a number
	
	of connections with mathematical finance and Bayesian nonparametrics.
	We
	
	argue that it provides an intuitive framework for generating new regularization
	
	penalties and shrinkage rules; for performing asymptotic analysis
	on existing
	
	models; and for simplifying proofs of some classic results on normal
	scale
	
	mixtures.},
  owner = {jrnold},
  timestamp = {2013.03.01}
}

@ARTICLE{RatkovicEng2010,
  author = {Ratkovic, Marc T. and Eng, Kevin H.},
  title = {Finding Jumps in Otherwise Smooth Curves: Identifying Critical Events
	in Political Processes},
  year = {2010},
  volume = {18},
  number = {1},
  pages = {57-77},
  doi = {10.1093/pan/mpp032},
  eprint = {http://pan.oxfordjournals.org/content/18/1/57.full.pdf+html},
  url = {http://pan.oxfordjournals.org/content/18/1/57.abstract},
  abstract = {Many social processes are stable and smooth in general, with discrete
	jumps. We develop a sequential segmentation spline method that can
	identify both the location and the number of discontinuities in a
	series of observations with a time component, while fitting a smooth
	spline between jumps, using a modified Bayesian Information Criterion
	statistic as a stopping rule. We explore the method in a large-n,
	unbalanced panel setting with George W. Bush's approval data, a small-n
	time series with median DW-NOMINATE scores for each Congress over
	time, and a series of simulations. We compare the method to several
	extant smoothers, and the method performs favorably in terms of visual
	inspection, residual properties, and event detection. Finally, we
	discuss extensions of the method.},
  journal = {Political Analysis},
  owner = {jrnold},
  timestamp = {2013.03.01}
}

@BOOK{ShumwayStoffer2010,
  author = {Shumway, R.H. and Stoffer, D.S.},
  title = {Time Series Analysis and Its Applications},
  year = {2010},
  series = {Springer Texts in Statistics},
  publisher = {Springer},
  isbn = {9781441978653},
  url = {http://books.google.com/books?id=NIhXa6UeF2cC},
  owner = {jrnold},
  timestamp = {2013.02.01}
}

@ARTICLE{spirling2007bayesian,
  author = {Spirling, Arthur},
  title = {Bayesian Approaches for Limited Dependent Variable Change Point Problems},
  year = {2007},
  volume = {15},
  number = {4},
  pages = {387-405},
  doi = {10.1093/pan/mpm022},
  eprint = {http://pan.oxfordjournals.org/content/15/4/387.full.pdf+html},
  url = {http://pan.oxfordjournals.org/content/15/4/387.abstract},
  abstract = {Limited dependent variable (LDV) data are common in political science,
	and political methodologists have given much good advice on dealing
	with them. We review some methods for LDV âchange point problemsâ
	and demonstrate the use of Bayesian approaches for count, binary,
	and duration-type data. Our applications are drawn from American
	politics, Comparative politics, and International Political Economy.
	We discuss the tradeoffs both philosophically and computationally.
	We conclude with possibilities for multiple change point work.},
  journal = {Political Analysis},
  owner = {jrnold},
  timestamp = {2011-04-02}
}

@ARTICLE{Strawderman1971,
  author = {Strawderman, William E.},
  title = {Proper Bayes Minimax Estimators of the Multivariate Normal Mean},
  year = {1971},
  language = {English},
  volume = {42},
  number = {1},
  pages = {pp. 385-388},
  issn = {00034851},
  url = {http://www.jstor.org/stable/2958496},
  copyright = {Copyright © 1971 Institute of Mathematical Statistics},
  journal = {The Annals of Mathematical Statistics},
  jstor_articletype = {research-article},
  jstor_formatteddate = {Feb., 1971},
  owner = {jrnold},
  publisher = {Institute of Mathematical Statistics},
  timestamp = {2013.04.24}
}

@ARTICLE{Tipping2001,
  author = {Tipping, Michael E.},
  title = {Sparse bayesian learning and the relevance vector machine},
  year = {2001},
  volume = {1},
  month = sep,
  pages = {211--244},
  issn = {1532-4435},
  doi = {10.1162/15324430152748236},
  url = {http://dx.doi.org/10.1162/15324430152748236},
  abstract = {This paper introduces a general Bayesian framework for obtaining sparse
	solutions to regression and classification tasks utilising models
	linear in the parameters. Although this framework is fully general,
	we illustrate our approach with a particular specialisation that
	we denote the 'relevance vector machine' (RVM), a model of identical
	functional form to the popular and state-of-the-art 'support vector
	machine' (SVM). We demonstrate that by exploiting a probabilistic
	Bayesian learning framework, we can derive accurate prediction models
	which typically utilise dramatically fewer basis functions than a
	comparable SVM while offering a number of additional advantages.
	These include the benefits of probabilistic predictions, automatic
	estimation of 'nuisance' parameters, and the facility to utilise
	arbitrary basis functions (e.g. non-'Mercer' kernels). We detail
	the Bayesian framework and associated learning algorithm for the
	RVM, and give some illustrative examples of its application along
	with some comparative benchmarks. We offer some explanation for the
	exceptional degree of sparsity obtained, and discuss and demonstrate
	some of the advantageous features, and potential extensions, of Bayesian
	relevance learning.},
  acmid = {944741},
  issue_date = {9/1/2001},
  journal = {J. Mach. Learn. Res.},
  numpages = {34},
  owner = {jrnold},
  publisher = {JMLR.org},
  timestamp = {2013.04.24}
}

@BOOK{WestHarrison1997,
  author = {West, M. and Harrison, J.},
  title = {{Bayesian forecasting and dynamic models}},
  year = {1997},
  series = {Springer series in statistics},
  publisher = {Springer},
  isbn = {9780387947259},
  url = {http://books.google.com/books?id=jcl8lD75fkYC},
  file = {west1997bayesian.pdf:west1997bayesian.pdf:PDF},
  lccn = {96038166},
  owner = {jrnold},
  timestamp = {2010.11.19}
}

