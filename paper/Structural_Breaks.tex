\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{fancyvrb}
% \usepackage{color}
\usepackage[latin1]{inputenc}
\usepackage[style=authoryear]{biblatex}
\usepackage{graphicx}
\usepackage{subcaption}
%\addbibresource{default}
\addbibresource{local}
\usepackage{setspace}
\doublespace

\usepackage{todo}

\author{Jeffrey B. Arnold}
% Seeing a Shrink about Structural Breaks
\title{Sparse State Distrurbance Dynamic Linear Models for Structural Breaks}


\newcommand{\dist}[1]{\mathcal{#1}}
\newcommand{\paren}[1]{\ensuremath{\left(#1\right)}}
\newcommand{\dnorm}[1]{\ensuremath{\dist{N}\paren{#1}}}
\newcommand{\dmvnorm}[2]{\ensuremath{\dist{N}_{#2}\paren{#1}}}
\newcommand{\dt}[2]{\ensuremath{\dist{T}_{#1}\paren{#2}}}
\newcommand{\dcauchy}[1]{\ensuremath{\dist{C}\paren{#1}}}
\newcommand{\dhalfcauchy}[1]{\ensuremath{\dist{C}^{+}\paren{#1}}}
\newcommand{\dbeta}[1]{\ensuremath{\dist{B}\paren{#1}}}
\newcommand{\dinvbeta}[1]{\ensuremath{\dist{IB}\paren{#1}}}
\newcommand{\dgamma}[1]{\ensuremath{\dist{G}\paren{#1}}}
\newcommand{\dinvgamma}[1]{\ensuremath{\dist{IG}\paren{#1}}}
\newcommand{\dwishart}[1]{\ensuremath{\dist{W}\paren{#1}}}
\newcommand{\dinvwishart}[1]{\ensuremath{\dist{IW}\paren{#1}}}
\newcommand{\dunif}[1]{\ensuremath{\dist{U}\paren{#1}}}

\newcommand{\RLang}{\textsf{R}}
\newcommand{\Stan}{Stan}
\newcommand{\R}{\ensuremath{\mathbb{R}}} %real

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\newcommand{\tran}{^\top}

\include{pygments}
\begin{document}

\maketitle{}

\begin{abstract}
  In estimating problems with time-varying parameters, researchers often have to choose between methods that smooth the change over time and methods that model the change as discrete breaks.
  This paper proposes using dynamic linear models with scale-mixture of gaussians a model time-varying processes that can account for either or both smoothly time-varying processes and processes with large discrete jumps.
  The problem of estimating time-varying parameters is a special case of the ``large-p'' problem, and this paper applies recent advances in that literature to the time-varying parameter problem.
  This provides a robust and flexible method. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Political and social processes are rarely, if ever, constant over time.
Moreover, many of these processes the changes in the process are characterized by many periods of stability and a few periods of possibly rapid and large, change \parencite{RatkovicEng2010}.
Because most smoothing methods such as splines and Kalman filters cannot account for the sparsity and scale differences in the parameter changes
\footnote{\textcite{RatkovicEng2010} is the notable exception in that their method combines both smoothly varying sections with structural breaks.}
structural break (change point) models are often used \parencites{CalderiaZorn1998}{Spirling2007}{Spirling2007a}{Park2010}{Park2011}.%

This paper presents a simple and flexible method to estimate time-varying parameters that may be subject to structural breaks.
Time-varying parameters with possible structural breaks can be estimated within the continuous state-space framework by using a shrinkage prior on the distribution of the state disturbances.
Suppose there is a sequence of data $y_{1}, \dots, y_{n}$ drawn from a normal distribution with a time varying mean, $\alpha_{1:n}$. This can be represented within a state space model as follows,
\begin{align}
  \label{eq:4}
  y_{t} &= \alpha_{t} + \varepsilon_{t} & \varepsilon_{t} \sim N(0, \sigma^{2}) \\
  \alpha_{t + 1} &= \alpha_{t} + \eta_{t}
\end{align}
A data generating process with a structural break is simply one in which the state disturbance vector, $\eta$, is sparse, meaning most $\eta_{t}$ are 0.
Estimating sparse parameters is a general problem that has received much attention lately, and this paper takes advantage of advances in that literature.
Estimating $\alpha$ in which there are possible structural breaks can be done by placing a shrinkage prior on $\eta$.
This paper will use the Horseshoe Prior distribution introduced in \textcite{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}.

This method has several appealing features.
First, while structural breaks and smoothly varying parameters are often treated with seperate models and researchers forced to choose between them, the can be nested within the continuous state space model.
They simply differ in the prior distribution (or equivalently penalty) they place on the state disturbances.
Second, and following from the nesting of structural breaks and smoothing methods, this method also does not require the researcher to specify \textit{ex ante} the number of structural breaks.
But, If the researcher has prior information about the number of structural breaks (or equivalently, the sparsity of the state disturbances)
Third, because this method is a subclass of the dynamic linear models, it inherits much of the flexibility of those models.
With only minor modifications, this method can be applied to problems in which the structural break is in the slope, regression coefficients, seasonal and cylical effects.
This allows the researcher to focus on specifying a model that makes sense rather than adjusting the model to one in which there is an existing test.
Fourth, because most shrinkage prior distributions, including the Horseshoe Prior distribution used in this paper, this state space part of this model can still be sampled using the computationally efficient methods developed for Gaussian Dynamic Linear Models.
This paper shows how these models can be estimated using \Stan and \RLang.

\section{Dynamic Linear Models}
\label{sec:dynam-line-models}

A Dynamic Linear Model (DLM), also called a Linear Gaussian state space model, for a $n$-dimensional observation sequence $y_{1}, \dots, y_{n}$ is defined by the following set of equations.%
\footnote{This paper follows the notation used in \textcite{DurbinKoopman2001}. See \textcite{PetrisPetroneEtAl2009} for a concordance with the notation used in \textcite{WestHarrison1997}.}
For $t = 1:n$,
\begin{align}
  \label{eq:8}
  \underset{p \times 1}{y_t} &= \underset{p \times m}{Z_{t}} \, \underset{m \times 1}{\alpha_t} + \underset{p \times 1}{\varepsilon_t} & \varepsilon_{t} &\sim \dmvnorm{0, H_{t}}{p} \\
  \label{eq:14}
  \underset{m \times 1}{\alpha_{t+1}} &= \underset{m \times m}{T_{t}} \, \underset{m \times 1}{\alpha_{t}} + \underset{m \times r}{R_{t}}  \underset{r \times 1}{\eta_{t}} & \eta_{t} &\sim \dmvnorm{0, Q_{t}}{r} \\
  \label{eq:2}
  \alpha_{1} & \sim \dmvnorm{a_{1}, P_{1}}{m}
\end{align}

Equation \eqref{eq:8} is the \textit{observation equation} which relates the \textit{observation vector} $y_{t}$ to the \textit{state vector} $\alpha_{t}$.
Equation \eqref{eq:14} is the \textit{state equation} which describes the Markovian evolution of the state vector.
Equation \eqref{eq:2} is the \textit{initial state equation} which is a prior distribution for the initial state $\alpha_{1}$.
The vectors $\varepsilon_{t}$ and $\eta_{t}$ are referred to as the \textit{observation} and \textit{state disturbances}, respectively.
The matrices $Z_{t}$, $H_{t}$, $T_{t}$, $R_{t}$, and $Q_{t}$ are referred to as the \textit{system matrices}.
The matrix $Z_{t}$ is the design matrix, $T_{t}$ is the transition matrix, $H_{t}$ is the observation covariance matrix, and $R_{t} Q_{t} R'_{t}$ is the state covariance matrix.
For the purposes of the estimation of $\alpha$, they are considered fixed and known, but in a larger model they can include parameters to be estimated.
Let $p$ be the dimension of the observation vector (number of variables), $m$ be the dimension of the state vector, and $r$ be the dimension of the state disturbances.
%then the dimensions of the elements in the DLM are show in Table \ref{tab:state_space_dim}.
% \begin{table}[!]
%   \centering
% \begin{tabular}{llll}
%   Vectors & dimension & Matrices & dimension \\
% \hline
%   $y_t$     & $p, 1$ & $Z_{t}$  & $p, m$ \\
%   $\alpha_{t}$ & $m, 1$ & $H_{t}$  & $p, p$ \\
%   $\varepsilon_t$ & $p, 1$ & $T_{t}$ & $m, m$ \\
%   $\eta$ & $r, 1$ & $R_t$ & $m, r$ \\
%    &  & $Q_t$ & $r, r$ \\
%   $a_{1}$ & $m, 1$ & $P_{1}$ & $m, m$
% \end{tabular}
%   \caption{The imensions of the elements in DLM (Equations \eqref{eq:8}, \eqref{eq:14}, and \eqref{eq:2})}
%   \label{tab:state_space_dim}
% \end{table}

DLMs nest a large number of common models, including ARIMA, stochastic volatility, (time-varying parameter) regressions,
and cubic splines \parencites{WestHarrison1997}{DurbinKoopman2001}\parencite{PetrisPetroneEtAl2009}{CommandeurKoopman2007}.%
\footnote{See \textcite{CommandeurKoopmanOoms2011} for a review of statistical software to estimate state space models.}
This flexibility is important because the methods presented below can be applied to a wide range of models with little modification.

\subsection{Structural Breaks in a State Space Model}
\label{sec:struct-breaks-state}

For simplicity of exposition, I will restrict my attention to the case of a single univariate observation vector $y_{1:n}$, with a time-varying mean.
This can be represented as a dynamic linear model as follows,
\begin{equation}
  \label{eq:5}
  \begin{aligned}[t]
    y_{t} &= \alpha_{t} + \varepsilon_{t} & \varepsilon & \sim \dnorm{0, H_{t}} \\
    \alpha_{t + 1} &= \alpha_{t} + \eta_{t} & \eta & \sim \dnorm{0, Q_{t}}
  \end{aligned}
\end{equation}
Equation \eqref{eq:5} is simply a DLM with $p = m = r = 1$, and $Z_{t} = T_{t} = 1$.
The model in Equation \eqref{eq:5} is commonly called a \textit{local level model} or a \textit{random walk with noise}. (Add citations to textbooks)

In equation \eqref{eq:5}, changes in the state are equal to the value of the state disturbance $\eta_{t} = \alpha_{t+1} - \alpha_{t}$. 
Thus the pattern of changes is going to be determined by the distribution on $\eta_{t}$.
In many problems in political science, the evolution of a parameter is expectedffer from to to be stable for long periods of time, with a few, possibly large, changes (the structural breaks).
Translated into the \eqref{eq:5}, this means that the researcher expects that the vector $\eta$ is sparse. 
In other words, for most periods $\eta_{t} = 0$, but there are a few periods in which $\eta_{t} \neq 0$, and, possibly,  $|\eta_{t}| \gg 0$.
When recast in this way, the problem of estimating structural breaks within a dynamic linear model is essentially a problem of estimating a sparse parameter vector $\eta$.
Estimating sparse parameter vectors, especially in the context of $p \gg n$, is a problem that has received and is currently receiving much attention.
I will refer to $\eta_{t} \approx 0$ as ``noise'', and $|\eta_{t}| \gg 0$ as ``signals''. 
In sparse estimation problems, the researcher wants to classify noise and signals; in the case of the structural breaks, the structural breaks are the signals, and the non-structural break periods are noise.

In Bayesian estimation, there are two main approaches for finding sparse solutions of a parameter vector: discrete mixtures and shrinkage priors.
The first approach is models each parameter with a prior consisting of a discrete mixture over a point mass at zero and a continuous distribution.
These a commonly called spike-and-slab priors.
In the context of estimating TVP, $\eta_{t}$ is given the following distribution,
\begin{equation}
  \label{eq:1}
  \eta_{t} = p \delta_{0} +  (1 - p) g(\eta_{t})
\end{equation}
where $p \in [0, 1]$, $\delta_{0}$ is the point mass distribution on 0, and $g(\eta_{t})$ is the distribution if $\eta \neq 0$.
\textcite{GiordaniKohn2008} propose using \eqref{eq:1} to model structural breaks within a continuous state space framework.

The second approach are shrinkage priors, which are absolutely continuous distributions centered at zero.
These shrinkage priors correspond to likelihood penalties; most notably, the popular Lasso/L1 regularization corresponds to \textit{maximum a posteriori} (MAP) estimation with a Laplacian (double exponential) prior on the parameters.
Examples of these priors include ... 
Recasting the problem in this way, the problem of estimating structural breaks within a dynamic linear model is essentially a problem of estimating a sparse parameter vector $\eta$.
Estimating sparse parameter vectors, especially in the context of $p \gg n$ (which many DLM's fall into), is a problem that has received and is currently receiving much attention.
\textit{Insert citations}

In Bayesian estimation, there are two classes of models for estimating sparse parameter vectors.
The first are \textit{selection} approaches, which model the sparse parameters with a discrete mixture of a point mass at zero and a continuous distribution.
In this framework a structural break model can be estimated 

\textcite{GiordaniKohn2008} 
The set of periods with structural breaks is selected, and then the value of the parameter within each region is estimated. 

The second is to use shrinkage priors, continuous distributions.

% A common approach is to assume that the innovations have a constant variance over time, $Q_{t} = Q$  for all $t \in 1:n$.
% If $\eta_{t} \sim N(0, Q)$ then large values of $\eta_{t}$ ($\alpha_{t} - \alpha_{t-1}$) are penalized and thus the  estimated $\eta_{t}$ are shrunk towards zero. 
% By penalizing large values of $\eta$, this smooths $\alpha$ over time.

For a shrinkage prior to work well it must,\parencite[5]{PolsonScott2010}
\begin{itemize}
\item $p(\lambda_{t}^{2})$ should have heavy tails,
\item $p(\tau^{2})$ should have a substantial mass near zero
\end{itemize}
Global shrinkage via $\tau$ handles shrinks noise, while local values of $\lambda_{t}$ act to detect the signals.\parencite[5]{PolsonScott2010}

Importantly for the computational efficiency of these approaches, almost all of these shrinkage priors can be represented as global-local mixture of normal distributions.
\begin{equation}
  \label{eq:3}
  \begin{aligned}[t]
    \eta_{t} &= N(0, \tau^{2} \lambda_{t}^{2})
    \lambda_{t}^{2} &\sim p(\lambda_{t}^{2})
  \end{aligned}
\end{equation}
The $\tau$ is referred to as the global variance component,%
\footnote{This is equivalent to the regularization parameter in penalized-likelihood.}
and the $\lambda_{t}$ are referred to as local variance components \parencite{PolsonScott2010}.

Note that the commonly used $\eta_{t} \sim N(0, Q)$ is trivially a scale-mixture of normals ($\lambda_{t}^{2} = \delta_{1}$).
However, it does not impose sparsity. 
The normal distribution does not have a large mass at zero, and thus undershrinks noise, and it has very thin tails, and thus overshrinks signals.

The idea of using the $t$-distribution to model structural breaks has been previously suggested in \textcite{HarveyKoopman2000}[184][]{DurbinKoopman2001}{PetrisPetroneEtAl2009} suggest using a $t$-distribution as a prior distribution for $\eta$ to model structural breaks.
While the $t$-distribution has heavy tails as it tends towards the Cauchy distribution, it does not have a substantial mass near zero, and thus will not shrink noise disturbances.

While there are an increasing number of proposed shrinkage priors, this paper will use the Horseshoe Prior distribution \parencites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}{PolsonScott2010}{PolsonScott2012}{DattaGhosh2012}.
The Horseshoe Prior distribution has no analytical form, but can be represented as a scale-mixture of normal distributions (as in \eqref{eq:3}) in which the local scale components $\lambda$ are distributed half-Cauchy,
\begin{equation}
  \label{eq:6}
  \lambda \sim \dhalfcauchy{0, 1}
\end{equation}
or equivalently, the local variance components are distributed inverse-beta,
\begin{equation}
  \label{eq:9}
  \lambda^{2} \sim \dinvbeta{\frac{1}{2}, \frac{1}{2}}
\end{equation}
The Horseshoe prior distribution has many appealing properties.
It has both an infinitely high spike at zero, and Cauchy-like tails.
This allows it to aggressively shrink noise towards zero, while leaving signals unshrunk.
Figure \ref{fig:horseshoe} compares the density function of the Horseshoe Prior to the normal, Cauchy, and Laplacian (double-exponential) distributions, both around zero and in the tails.
\begin{figure}
  \centering
  \includegraphics{plots/fig-horseshoe1.pdf}
  \includegraphics{plots/fig-horseshoe2.pdf}
  \caption{The density of the horseshoe prior distribution (in black) compared with the densities of the normal, Cauchy, and Laplacian distributions (in gray).}
  \label{fig:horseshoe}
\end{figure}

To summarize, the proposed local level model that is robust to structural breaks is as follows,
\begin{equation}
  \label{eq:10}
  \begin{aligned}[t]
    y_{t} &\sim N(\alpha_{t}, H) \\
    \alpha_{t + 1} &\sim N(\alpha_{t}, \tau^{2} \lambda^{2}_{t}) \\
    \lambda^{2}_{t} & \sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}
This model could be completed with the noninformative priors,
\begin{equation}
  \label{eq:7}
  \begin{aligned}[t]
    p(H) &= \frac{1}{H} \\
    \tau &\sim \dhalfcauchy{0, H}
  \end{aligned}
\end{equation}
$H$ is given its Jeffrey's prior, while $\tau$ is distributed Cauchy
on the same scale as $H$.

\subsection{Identifying Structural Breaks}

Structural breaks can be detected from the estimates of the  posterior distribution in a couple of ways.
The first method is simply to check whether the credible interval of the posterior distribution of the state disturbances $p(\eta_{t} | y, .)$ crosses zero.
This is the Bayesian equivalent of the auxiliary residual test in \textcite{JongPenzer1998}{DurbinKoopman2001}.%
Apart from other differences between credible and confidence intervals, the Bayesian estimate has the advantage of marginalizing over the posterior distribution of the system matrix parameters and thus accounting for the  uncertainty in the estimates of the other parameters.
The maximum likelihood method simply evaluates the auxiliary residuals at the mode of the other parameters.%
\footnote{Depending on the application it may be more appropriate to directly analyze $p(\alpha_{t} - \alpha_{t-1})$, which also includes the linear transformations in the system matrices $Z_{t}$ and $c_{t}$ which may include parameters.}
However, it may be better to calculate one sided probability statements, such as $\Pr(\eta_{t} | y > 0)$ or $\Pr(\eta_{t} | y <0)$.

The second method, suggested by \textcite[179-180]{PetrisPetroneEtAl2009} is to use $\lambda_{t}$ identify structural breaks.
If $\lambda_{t} = 1$, then the state disturbance is distributed normal with global scale $\eta_{t} \sim N(0, \tau)$.
Thus, the local shrinkage values $\lambda_{t}$ are a roughly measure of the shrinkage of $\eta_{t}$.
Values of $\lambda_{t} < 1$ are observations that are shrunk
Values of $\lambda_{t} > 1$ put a higher prior probability on larger absolute values of $\lambda_{t}$.

Alternatively, the researcher can choose not to try to classify the periods as structural breaks or not, but rather directly focus on the magnitudes of $\eta_{t}$.
For example, it may be of more practical interest to order the $\eta_{t}$ by their absolute values.

\subsection{Multivariate States}
\label{sec:multivariate}

Sparsity can also be imposed on the  state disturbances can also be extended to multivariate state spaces.
Scale mixture of normal distributions can be extended to the multivariate case as follows,%
\footnote{
  This seems intuitive, but I cannot find any one who has done this.
  I may need to characterize the properties of this class of distributions.
}
\begin{equation}
  \label{eq:12}
  \eta_{t} \sim \dmvnorm{0, \Lambda_{t} \Sigma \Lambda_{t}'}{m} \\
\end{equation}
where $\Sigma$ is the global covariance component, and $\Lambda_{t}$ is a lower triangular matrix such that 
$\Lambda_{t} \Lambda_{t}'$ is the local covariance matrix.

To get a multivariate version of the Horseshoe Prior distribution, decompose the local covariance matrix into a standard deviation vector and a correlation matrix,
\begin{align}
  \label{eq:16}
  \Lambda_{t} \Lambda_{t}' &= \diag(\lambda_{t}) R_{t} \diag(\lambda_{t})' \\
  \label{eq:17}
  \lambda_{t,i} &= \dhalfcauchy{0, 1} & \text{for $i \in 1:p$}
\end{align}
where $R_{t}$ is a correlation matrix.
Each element in $\lambda_{.,i}$ for all $t \in 1:n$ share a common prior distribution, shrinking the values across time.

\subsection{Outliers}
\label{sec:outliers}

Robustness to and detection of outliers is analagous to that of time-varying parameters.
The observation disturbances $\varepsilon$ are modeled by assigning a scale mixture of normal distributions to $H_{t}$.
However, unlike structural breaks, there is often less concern about the sparsity of $\varepsilon$ than the possibility of very large values of $\varepsilon$.
Thus, the $t$-distribution, which has fat tails, but not a spike at zero, will likely work well in many applications.

% \section{Monte Carlo}
% \label{sec:monte-carlo}

% Since this would be computationally expensive, I've been working on both Stan code and a new R package for Kalman filters in order to be able to do these.

\section{Examples}
\label{sec:examples}

\subsection{Nile Flow Data}
\label{sec:nile}

The first example is the Nile river flow data, which is a classic dataset in the  \parencites{Cobb1978}{Balke1993}{JongPenzer1998}{DurbinKoopman2001}{DurbinKoopman2012}
The data consist of annual observations of the flow of the Nile river at Ashwan between 1871 and 1970.
It is well known that there was a level shift in 1899, both due to the construction of a damn at Ashwan and weather changes.
Since in this example, there is only one clear change point, it is not fully exploiting the flexibility of this method, but is instead a sanity check.
It will illustrate important differences between the adaption of the HPDLM and GDLM to a structural break,
and it will show how, the HPDLM can approximate an intervention without any \textit{ex ante} input from the analyst.

I compare the performance of the horseshoe prior innovations model ($M_{nile,HS}$) with two alternative models.
The first model has a uses a normal distribution with a time-invariant variance for the innovations ($M_{nile,normal}$).
The second model extends $M_{nile,normal}$ to include a single parameter that represents change in the level after 1899 ($M_{nile,normal2}$).
The details of these models is given in Section \ref{sec:nile-1}.
Figure \ref{fig:nile} plots the original data, and the mean of the posterior predictive distributions for each of these models.
The  shows a sharp drop 1899 and stability before and after the break.
The normal model $M_{nile,normal}$ shows a smother adjustment with the decline in the level beginning a few years before 1899 and continuing a few years thereafter.
Model $M_{nile,normal}$ also shows more variability in the level before and after 1899.
This variability illustrates the importance of using a scale mixtures of normal distributions with local shrinkage parameters ($\lambda_{t}$) in addition to a global shrinkage parameter ($\tau$).
Since the normal model has only a single global shrinkage parameter ($\tau$). 
In order to accommodate the large change in the level in 1899, the estimated value of $\tau$ must increase.
However, increasing $\tau$ will result in less smoothing in the other observations.
The normal model must trade off shrinking the non-structural breaks and not shrinking the structural break with only a single parameter, resulting in over-smoothing around the break and under-smoothing elsewhere.

It is also remarkable that the horseshoe prior model's posterior predictive means closely match those of the intervention model ($M_{nile,normal2}$) without any \textit{ex ante} knowledge of the presence of the structural break in 1899.
There is a slight difference in the two models in that the horseshoe prior model puts some weight on the possibility that the structural break occurred in 1897 or 1898.
This is most likely due to the low signal to noise ratio in the data; note that the observations in 1897 and 1898 are consistent with, although high for, the distribution of flows after 1899.
The Nile model is an easy case in that the series has a single, large level change with a clear causal event, and thus easy to include a dummy variable.
However, in many applications, the presence of the structural break will not be known, and in fact estimating the presence and location of the structural breaks will be the purpose of the application.

Figures \ref{fig:nile_innovations} and \ref{fig:nile_w} show the results of the two methods that could be used to identify structural breaks. 
Figure \ref{fig:nile_innovations} plots the mean and 95 percent HPD interval of each $p(\omega_{t} | y)$.%
\footnote{For $M_{nile,normal2}$, the posterior distribution $p(\omega_{t} + \delta (x_{t} - x_{t-1}) | y)$ is used.}
The normal model $M_{nile,nomral}$ shows no structural breaks, while the intervention model $M_{nile,normal2}$ shows a clear structural break at 1899.
In the horseshoe model, the estimated mean of $p(\omega_{1899} | y)$ is large, suggesting a structural break, although its 95 percent credible interval does not cross zero.
As noted before, this seems to be due to small, although highly variable estimates, of $\omega$ in 1897 and 1898, suggesting small probabilities that the structural break occurred in those years.
Note that the observed data in those years is consistent with the upper tail of the distribution after 1899.
The reason that there is near certainty of a structural break in 1899 as opposed to the two earlier years is due to outside data, the knowledge that the dam was built in that year.
However, the second method, using the values of $w_{t}$ classifies 1899 as a structural break, giving the $\Pr(\omega_{1899} \neq 0) \approx 0.55$.

\begin{figure}[htpb]
  \centering
  \includegraphics{plots/fig-nile.pdf}
  \caption{Plot of mean posterior predictive distributions ($\E p(\tilde{y}| y)$) for the normal, normal2, and horseshoe prior distribution models.}
  \label{fig:nile}
\end{figure}

\begin{figure}[htpb]
  \centering
  \includegraphics{plots/fig-nile_innovations.pdf}
  \caption{Plot of innovations}
  \label{fig:nile_innovations}
\end{figure}

\begin{table}[htpb]
  \centering
  \input{plots/tab-nile.tex}
  \caption{Model summary statistics of Nile models.}
  \label{tab:nile}
\end{table}

\section{Implementation}
\label{sec:implementation}

% There are a wide variety of sofware that can estimate state space models.
% See the special issue of the \textit{Journal of Statistical Software} provides an overview of the software available to estimate state space models, including in R, Stata, and Matlab.%
% \footnote{Also see \textcite{Tusell2011} for a review of R packages for Kalman filtering and smoothing.}

Although state space models are often estimated using Bayesian MCMC methods, methods to efficiently sapmle from DLMs are not supported by most general purpose Bayesian software (OpenBUGS, JAGS, and PyMC).
While, the  equations of the dynamic linear model (~\eqref{eq:8} and \eqref{eq:14}) can be easily translated into the BUGS language, it is difficult to efficiently estimate the models.
That is because, BUGS and JAGS treat each $\alpha_{t}$ as a seperate parameter, and sample $\alpha_{t}$ conditional on the values of the other $\alpha_{-t}$.
Due to the high correlation between $\alpha_{t}$ and $\alpha_{t+1}$, this sampling procedure will generally result in poor mixing \parencite[477]{Jackman2009}.

To avoid this poor mixing, more efficient, specialized methods to sample from joint distribution of $\alpha_{1:n}$ have been developed.
A standard method for sampling from the joing distribution of $\alpha$ is the Forward-Filter Backward Sampler (FFBS), independently developed by \textcite{CarterKohn1994} and \textcite{Fruehwirth-Schnatter1994}.
Subsequently, more efficient sampling methods have been developed by \textcite{DeJongShephard1995}, \textcite{DurbinKoopman2002}, \textcite{StricklandTurnerDenhamEtAl2009}, and \textcite{ChanJeliazkov2009}.
However, the methods for sampling from the joing distribution of $\alpha$ are not included in the general purpose software (OpenBUGS, JAGS, PyMC), and thus a researcher interested in estimating dynamic linear models has had to write a custom samplers for each application.

This paper shows a method for easily and efficiently estimating the posterior distribution of the state space model using Stan and R.
This paper shows how Stan can be used to easily and efficiently estimate the posterior distribution of these models.
Like BUGS, Stan \textcite{Stan2013}{Stan2013a} is a general purpose Bayesian software with a domain specific language that hides the details of the sampling from the user.
Howeer, rather than using Gibbs Sampling like BUGS and JAGS, Stan implements a variant of Hamiltonian Monte Carlo (HMC) algorithm, the details of which are in \textcite{HoffmanGelman2013}.

The posterior distribution of a dynamic linear model can be sampled from using Stan and R.
This method proceeds in two steps.
\begin{enumerate}
\item First, the system matrices ($\mathcal{S}_{t} = \{T_{t}, Z_{t}, H_{t}, Q_{t}, R_{t}\}$, or rather any parameters in them are sampled within Stan. This can be done efficiently within Stan without sampling from $\alpha_{1:n}$ by adding the likelihood $p(y_{1:n} | \mathcal{S})$ to the log-posterior of the model.
The likelihood of $p(y_{1:n} | .)$ conditional on the system 
\item Second, to sample from $\alpha$, for each sample of the posterior distribution of $\mathcal{S}$, draw a sample of $\alpha$ using FFBS or a simulation smoother. In this paper, I use the R package \textbf{KFAS}.
\end{enumerate}

A useful feature of Stan's programming language is that custom distributions can be implemented by simply adding their contribution to the log posterior distribution.
In the case of a dynamic linear model, this means that to estimate parameters in the system matrices  $p(y_{1:n} | T_{1:n}, Z_{1:n}, Q_{1:n}, R_{1:n}, H_{1:n})$.
The likelihood of the DLM can be efficiently calculated using the Kalman filter. 
And the Kalman filter can be written in the Stan modeling language.

\subsubsection{Other Examples}

This paper needs some / good examples.

\begin{itemize}
\item Presidential approval for George W. Bush. \parencites{RatkovicEng2010}
\item Median ideal point of the Senate. \parencites{RatkovicEng2010}
\item Supreme court dissents and concurrences. 1 or 2 structural breaks. Poisson data. \parencite{CalderiaZorn1998}
\item Discrete DV change-point models in \parencite{spirling2007bayesian}.
\item Interest Rates, Inflation, and GDP growth are common economics examples, e.g. \textcite{GiordaniKohn2008}.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

This paper shows that a simple tweak to dynamic linear models allow them to estimat structural break models with a random number of breaks.
The sparse disturbance approach is both intuitive and flexible, while remaining computationally efficient.

\clearpage{}
\section{Appendix}
\label{sec:appendix}

\subsection{Models}
\label{sec:models}

\subsubsection{Nile}
\label{sec:nile-1}

Model $M_{nile,normal}$ is a local level model with a normal distribution.
The observation variance is given an improper Jeffrey's prior.
The system variance (global scale parameter) is given a half-Cauchy distribution. 
The initial state is given a semi-informative prior, a normal distribution with a mean at the value of the first observation, and variance equal to the sample variance of the data.
\begin{equation}
  \label{eq:11}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\theta_{t}, \sigma^{2}} \\
    \theta_{t} &\sim \dnorm{\theta_{t - 1}, \sigma^{2} \tau^{2}} \\
    p(\sigma^{2}) &= \frac{1}{\sigma^{2}} \\
    \tau &\sim \dhalfcauchy{0, 1} \\
    p(\theta_{1}) &\sim \dnorm{y_{1}, \Var{y}}
  \end{aligned}
\end{equation}

Model $M_{nile,HS}$ differs from $M_{nile,normal}$ in that it assumes a horseshoe prior distribution on 
the innovations,
\begin{equation}
  \label{eq:18}
  \begin{aligned}[t]
    \theta_{t} &\sim \dnorm{\theta_{t - 1}, \sigma^{2} \lambda_{t}^{2} \tau^{2}} \\
    \lambda &\sim \dhalfcauchy{0, 1} \\
    \tau &\sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}

Model $M_{nile,HS}$ differs from $M_{nile,normal}$ by adding an intervention parameter $\delta$ to the observation equation to model the level shift. 
The data $x_{t}$ is a binary vector equal to 0 before 1899, and 1 thereafter.
\begin{equation}
  \label{eq:19}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\theta_{t} + \delta x_{t}, \sigma^{2}} \\
    \delta &\sim \dunif{-\infty, \infty}
  \end{aligned}
\end{equation}

\clearpage{}

\printbibliography{}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

%  LocalWords:  Carvallho
