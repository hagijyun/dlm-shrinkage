\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{fancyvrb}
% \usepackage{color}
\usepackage[latin1]{inputenc}
\usepackage[style=authoryear]{biblatex}
\usepackage{graphicx}
\usepackage{subcaption}
\addbibresource{default}
\usepackage{setspace}
\doublespace

\usepackage{todo}

\author{Jeffrey B. Arnold}
% Seeing a Shrink about Structural Breaks
\title{Sparse State Distrurbance Dynamic Linear Models for Structural Breaks}

\newcommand{\paren}[1]{\ensuremath{\left(#1\right)}}
\newcommand{\dnorm}[1]{\ensuremath{\mathcal{N}\paren{#1}}}
\newcommand{\dmvnorm}[2]{\ensuremath{\mathcal{N}_{#2}\paren{#1}}}
\newcommand{\dt}[2]{\ensuremath{\mathcal{T}_{#1}\paren{#2}}}
\newcommand{\dcauchy}[1]{\ensuremath{\mathcal{C}\paren{#1}}}
\newcommand{\dhalfcauchy}[1]{\ensuremath{\mathcal{C}^{+}\paren{#1}}}
\newcommand{\dbeta}[1]{\ensuremath{\mathcal{B}\paren{#1}}}
\newcommand{\dinvbeta}[1]{\ensuremath{\mathcal{IB}\paren{#1}}}
\newcommand{\dgamma}[1]{\ensuremath{\mathcal{G}\paren{#1}}}
\newcommand{\dinvgamma}[1]{\ensuremath{\mathcal{IG}\paren{#1}}}
\newcommand{\dwishart}[1]{\ensuremath{\mathcal{W}\paren{#1}}}
\newcommand{\dinvwishart}[1]{\ensuremath{\mathcal{IW}\paren{#1}}}
\newcommand{\dunif}[1]{\ensuremath{\mathcal{U}\paren{#1}}}

\newcommand{\RLang}{\textsf{R}}
\newcommand{\R}{\ensuremath{\mathbb{R}}} %real

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}

\include{pygments}
\begin{document}

\maketitle{}

\begin{abstract}
  In estimating problems with time-varying parameters, researchers often have to choose between methods that smooth the change over time and methods that model the change as discrete breaks.
  This paper proposes using dynamic linear models with scale-mixture of gaussians a model time-varying processes that can account for either or both smoothly time-varying processes and processes with large discrete jumps.
  The problem of estimating time-varying parameters is a special case of the ``large-p'' problem, and this paper applies recent advances in that literature to the time-varying parameter problem.
  This provides a robust and flexible method. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Political and social processes are rarely, if ever, constant over time. 
Moreover, many of these processes the changes in the process are characterized by many periods of stability and a few periods of possibly rapid and large, change \parencite{RatkovicEng2010}.
Because many smoothing methods such as splines and Kalman filters cannot account for the sparsity and scale differences in the innovation process,
\footnote{\textcite{RatkovicEng2010} is the notable exception in that their method combines both smoothly varying sections with structural breaks.}
structural break (change point) models are often used \parencites{CalderiaZorn1998}{Spirling2007}{Spirling2007a}{Park2010}{Park2011}.%

This paper shows how both structural break and smoothing processes are nested within continuous state space models.
Structural break models and smoothing models differ in the assumptions they make over the distribution (or equivalently the penalty) on the innovations, the changes in the parameters.
In structural break models, the innovations are modeled as a discrete mixture distribution between a point mass at zero (the non-breaks) and an often diffuse distribution for the non-zero innovations (the structural breaks).
Smoothing models model the innovations with a continuous distribution that shrinks the innovations toward zero.
The problem with most smoothing models in the face of sparse innovations is that the normal distribution does not shrink the non-breaks enough to zero, and they shrink the structural break innovations too much.
Noting that identifying the structural breaks among the innovations is an example of a multiple testing problem, this paper draws on recently developed Bayesian shrinkage priors
These shrinkage priors are able more aggressively shrink non-signals towards zero while leaving signals unshrunk.
In particular, this paper will use the horseshoe prior distribution introduced in \textcite{CarvalhoPolsonScott2010} and \textcite{CarvalhoPolsonScott2009} to model the innovations.
Although the horseshoe prior distribution is continuous, it behaves quite similarly to the discrete mixture model.

\section{Dynamic Linear Models}
\label{sec:dynam-line-models}

A Dynamic Linear Model (DLM), also called a Linear Gaussian state space model, for a $n$-dimensional observation sequence $y_{1}, \dots, y_{n}$ is defined by the following set of equations.%
\footnote{This paper follows the notation used in \textcite{DurbinKoopman2001}. See \textcite{PetrisPetroneEtAl2009} for a concordance with the notation used in \textcite{WestHarrison1997}.}
For $t = 1:n$,
\begin{align}
  \label{eq:8}
  y_t &= Z_{t} \alpha_t + \varepsilon_t & \varepsilon_{t} &\sim \dmvnorm{0, H_{t}}{p} \\
  \label{eq:14}
  \alpha_{t+1} &= T_{t} \alpha_{t} + R_{t} \eta_{t} & \eta_{t} &\sim \dmvnorm{0, Q_{t}}{r} \\
  \label{eq:2}
  \alpha_{1} & \sim \dmvnorm{a_{1}, P_{1}}{m}
\end{align}

Equation \eqref{eq:8} is the \textit{observation equation} which relates the \textit{observation vector} $y_{t}$ to the \textit{state vector} $\alpha_{t}$.
Equation \eqref{eq:14} is the \textit{state equation} which describes the Markovian evolution of the state vector.
Equation \eqref{eq:2} is the \textit{initial state equation} which is a prior distribution for the initial state $\alpha_{1}$.
The vectors $\varepsilon_{t}$ and $\eta_{t}$ are referred to as the \textit{observation} and \textit{state disturbances}, respectively.
The matrices $Z_{t}$, $H_{t}$, $T_{t}$, $R_{t}$, and $Q_{t}$ are referred to as the \textit{system matrices}.
The matrix $Z_{t}$ is the design matrix, $T_{t}$ is the transition matrix, $H_{t}$ is the observation covariance matrix, and $R_{t} Q_{t} R'_{t}$ is the state covariance matrix.
For the purposes of the estimation of $\alpha$, they are considered fixed and known, but in a larger model they can include parameters to be estimated.
Let $p$ be the dimension of the observation vector (number of variables), $m$ be the dimension of the state vector, and $r$ be the dimension of the state disturbances, then the dimensions of the elements in the DLM are show in Table \ref{tab:state_space_dim}.
\begin{table}[!]
  \centering
\begin{tabular}{llll}
  Vectors & dimension & Matrices & dimension \\
\hline
  $y_t$     & $p, 1$ & $Z_{t}$  & $p, m$ \\
  $\alpha_{t}$ & $m, 1$ & $H_{t}$  & $p, p$ \\
  $\varepsilon_t$ & $p, 1$ & $T_{t}$ & $m, m$ \\
  $\eta$ & $r, 1$ & $R_t$ & $m, r$ \\
   &  & $Q_t$ & $r, r$ \\
  $a_{1}$ & $m, 1$ & $P_{1}$ & $m, m$
\end{tabular}
  \caption{The imensions of the elements in DLM (Equations \eqref{eq:8}, \eqref{eq:14}, and \eqref{eq:2})}
  \label{tab:state_space_dim}
\end{table}

DLMs nest a large number of common models, including ARIMA, stochastic volatility, (time-varying parameter) regressions,
and cubic splines \parencites{WestHarrison1997}{DurbinKoopman2001}\parencite{PetrisPetroneEtAl2009}{CommandeurKoopman2007}.%
\footnote{See \textcite{CommandeurKoopmanOoms2011} for a review of statistical software to estimate state space models.}
This flexibility is important because the methods presented below can be applied to a wide range of models with little modification.

\subsection{Structural Breaks in a State Space Model}
\label{sec:struct-breaks-state}

For simplicity of exposition, I will restrict my attention to the case of a single univariate observation vector $y_{1:n}$, with a time-varying mean.
This can be represented as a dynamic linear model as follows,
\begin{equation}
  \label{eq:5}
  \begin{aligned}[t]
    y_{t} &= \alpha_{t} + \varepsilon_{t} & \varepsilon & \sim \dnorm{0, H_{t}}
    alpha_{t + 1} &= \alpha_{t} + \eta_{t} & \varepsilon & \sim \dnorm{0, Q_{t}}
  \end{aligned}
\end{equation}
Equation \eqref{eq:5} is simply a DLM with $p = m = r = 1$, and $Z_{t} = T_{t} = 1$.
The model in Equation \eqref{eq:5} is commonly called a \textit{local level model} or a \textit{random walk with noise}. (Add citations to textbooks)

In equation \eqref{eq:5}, changes in the state are equal to the value of the state disturbance $\eta_{t} = \alpha_{t+1} - \alpha_{t}$. 
Thus the pattern of changes is going to be determined by the distribution on $\eta_{t}$.
In many problems in political science, the evolution of a parameter is expectedffer from to to be stable for long periods of time, with a few, possibly large, changes (the structural breaks).
Translated into the \eqref{eq:5}, this means that the researcher expects that the vector $\eta$ is sparse. 
In other words, for most periods $\eta_{t} = 0$, but there are a few periods in which $\eta_{t} \neq 0$, and, possibly,  $|\eta_{t}| \gg 0$.
When recast in this way, the problem of estimating structural breaks within a dynamic linear model is essentially a problem of estimating a sparse parameter vector $\eta$.
Estimating sparse parameter vectors, especially in the context of $p \gg n$, is a problem that has received and is currently receiving much attention.
I will refer to $\eta_{t} \approx 0$ as ``noise'', and $|\eta_{t}| \gg 0$ as ``signals''. 
In sparse estimation problems, the researcher wants to classify noise and signals; in the case of the structural breaks, the structural breaks are the signals, and the non-structural break periods are noise.

In Bayesian estimation, there are two main approaches for finding sparse solutions of a parameter vector: discrete mixtures and shrinkage priors.
The first approach is models each parameter with a prior consisting of a discrete mixture over a point mass at zero and a continuous distribution.
These a commonly called spike-and-slab priors.
In the context of estimating TVP, $\eta_{t}$ is given the following distribution,
\begin{equation}
  \label{eq:1}
  \eta_{t} &= p \delta_{0} +  (1 - p) g(\eta_{t})
\end{equation}
where $p \in [0, 1]$, $\delta_{0}$ is the point mass distribution on 0, and $g(\eta_{t})$ is the distribution if $\eta \neq 0$.
\textcite{GiordaniKohn2008} propose using \eqref{eq:1} to model structural breaks within a continuous state space framework.

<<<<<<< HEAD
The second approach are shrinkage priors, which are absolutely continuous distributions centered at zero.
These shrinkage priors correspond to likelihood penalties; most notably, the popular Lasso/L1 regularization corresponds to \textit{maximum a posteriori} (MAP) estimation with a Laplacian (double exponential) prior on the parameters.
Examples of these priors include ... 
=======
Recasting the problem in this way, the problem of estimating structural breaks within a dynamic linear model is essentially a problem of estimating a sparse parameter vector $\eta$.
Estimating sparse parameter vectors, especially in the context of $p \gg n$ (which many DLM's fall into), is a problem that has received and is currently receiving much attention.
\textit{Insert citations}

In Bayesian estimation, there are two classes of models for estimating sparse parameter vectors.
The first are \textit{selection} approaches, which model the sparse parameters with a discrete mixture of a point mass at zero and a continuous distribution.
In this framework a structural break model can be estimated 


\textcite{GiordaniKohn2008} 
The set of periods with structural breaks is selected, and then the value of the parameter within each region is estimated. 

The second is to use shrinkage priors, continuous distributions.

% A common approach is to assume that the innovations have a constant variance over time, $Q_{t} = Q$  for all $t \in 1:n$.
% If $\eta_{t} \sim N(0, Q)$ then large values of $\eta_{t}$ ($\alpha_{t} - \alpha_{t-1}$) are penalized and thus the  estimated $\eta_{t}$ are shrunk towards zero. 
% By penalizing large values of $\eta$, this smooths $\alpha$ over time.

>>>>>>> 19ef1453abd877df9e41b93109e9d335ddd6bc7d
What is needed to be able to capture sparse innovations $\eta$ is a distribution with 
\begin{enumerate}
\item an infinitely high spike near 0
\item fat Cauchy like tails
\end{enumerate}

Importantly for the computational efficiency of these approaches, almost all of these shrinkage priors can be represented as global-local mixture of normal distributions.
\begin{equation}
  \label{eq:3}
  \eta_{t} &= N(0, \tau \lambda_{t})
  \lambda_{t} &\sim p(\lambda_{t})
\end{equation}
The $\tau$ is referred to as the global variance component,%
\footnote{This is equivalent to the regularization parameter in penalized-likelihood.}
and the $\lambda_{t}$ are referred to as local variance components \parencite{PolsonScott2010}.

For a shrinkage prior to work well it must,\parencite[5]{PolsonScott2010}
\begin{itemize}
\item $p(\lambda_{t}^{2})$ should have heavy tails,
\item $p(\tau^{2})$ should have a substantial mass near zero
\end{itemize}
Global shrinkage via $\tau$ handles shrinks noise, while local values of $\lambda_{t}$ act to detect the signals.\parencite[5]{PolsonScott2010}

The common use of $Q_{t} = Q$ is one example of a shrinkage prior, in which the $\eta_{t}$ share a normal prior. 
However, this performs poorly under those criteria. 
The normal distribution does not have a large mass at zero, and thus undershrinks noise, and it has very thin tails, and thus overshrinks signals.

The $t$-distribution is also an example of a scale-mixture of normal distributions and a shrinkage prior.
The use of the $t$-distribution has a long history in robust Bayesian statistics and \textcite\textcite[184][]{DurbinKoopman2001}{PetrisPetroneEtAl2009} suggest using a $t$-distribution as a prior distribution for $\eta$ to model structural breaks.
While the $t$-distribution has heavy tails, it does not have a substantial mass near zero, and thus will undershrink noise.

Discussion of the Horseshoe prior distribution

% \begin{figure}
%   \centering
%   \includegraphics{plots/fig-horseshoe1.pdf}
%   \includegraphics{plots/fig-horseshoe2.pdf}
%   \caption{The density of the horseshoe prior distribution (in black) compared with the densities of the normal, Cauchy, and Laplacian distributions (in gray).}
%   \label{fig:horseshoe}
% \end{figure}

<<<<<<< HEAD
This is not to suggest that the Horseshoe Prior is the best. 
There have been a recent plethora of shrinkage priors, but formal properties of these are lagging.
This paper will show that the Horseshoe Prior performs reasonably well in real situations and Monte-Carlos.

\subsection{Identifying Structural Breaks}
=======
\subsection{Identifying Structural Breaks}

The probability of a structural breaks can be estimated from the posterior distribution of $\eta$.

The first method is not try to identify structural breaks ($p(\eta \neq 0)$) at all.
Instead, the inference should be on the effect sizes.

The second method.
\textcite{DurbinKoopman2012} suggest identifying structural breaks from from the auxiliary residuals of the state disturbances (see also, \textcite{JongPenzer1998})
The \textit{auxiliary residuals} of the state innovations are defined as,
\begin{equation}
  \label{eq:12}
  e^{*}_{t} = \frac{\hat \eta_{t}}{\Var (\hat \eta_{t})}
\end{equation}
where $\hat \eta_{t}$ and $\Var \hat \eta_{t}$ is the mean and variance of the smoothed esitmate of $\eta$, $\eta_{t} | y_{1:T}$.
Each auxiliary residual can be considered a t-test that there was no unobserved structural change in the systemtic component of the time series.
Thus a rule for identifying structural breaks using a 95\% confidence interval is if $|e^{*}_{t}| > 1.96$.
>>>>>>> 19ef1453abd877df9e41b93109e9d335ddd6bc7d

The probability of a structural breaks can be estimated from the posterior distribution of the estimates in a couple of ways.
The first method is to use the posterior distribution of $p(\eta_{t} | y, .)$.
Observations with a 95\% HPD credible interval which excludes zero can be categorized as structural breaks.
This is the Bayesian equivalent of the auxiliary residual test in \textcite{JongPenzer1998}{DurbinKoopman2001}.%
Apart from other differences between credible and confidence intervals, the Bayesian estimate has the advantage of marginalizing over the posterior distribution of the system matrix parameters and thus accounting for the  uncertainty in the estimates of the other parameters.
The maximum likelihood method simply evaluates the auxiliary residuals at the mode of the other parameters.%
\footnote{Depending on the application it may be more appropriate to directly analyze $p(\alpha_{t} - \alpha_{t-1})$, which also includes the linear transformations in the system matrices $Z_{t}$ and $c_{t}$ which may include parameters.}

The second method, suggested by \textcite[179-180]{PetrisPetroneEtAl2009} is to use $\lambda_{t}$ identify structural breaks.
If $\lambda_{t} = 1$, then the state disturbance is distributed normal with global scale $\eta_{t} \sim N(0, \tau)$.
Thus, the local shrinkage values $\lambda_{t}$ are a roughly measure of the shrinkage of $\eta_{t}$.
Values of $\lambda_{t} < 1$ are observations that are shrunk
Values of $\lambda_{t} > 1$ put a higher prior probability on larger absolute values of $\lambda_{t}$.

Alternatively, the researcher can choose not to try to classify the periods as structural breaks or not, but rather directly focus on the magnitudes of $\eta_{t}$

The third method is to note that if $\lambda_{t} = 1$, then the state disturbance is distributed normal with global scale $\eta_{t} \sim N(0, \tau)$.
Thus the local shrinkage values $\lambda_{t}$ are a measure of the non-normality of $\eta_{t}$.
Values of $\lambda_{t} < 1$ are underdispersed, and those with $\lambda_{t} > 1$ are overdispersed and likely structural breaks.

\section{Monte Carlo}
\label{sec:monte-carlo}

Since this would be computationally expensive, I've been working on both Stan code and a new R package for Kalman filters in order to be able to do these.

\section{Examples}
\label{sec:examples}

% \subsection{Nile Flow Data}
% \label{sec:nile}

% The first example is the Nile river flow data, which is a classic dataset in the  \parencites{Cobb1978}{Balke1993}{JongPenzer1998}{DurbinKoopman2001}{DurbinKoopman2012}
% The data consist of annual observations of the flow of the Nile river at Ashwan between 1871 and 1970.
% It is well known that there was a level shift in 1899, both due to the construction of a damn at Ashwan and weather changes.
% Since in this example, there is only one clear change point, it is not fully exploiting the flexibility of this method, but is instead a sanity check.
% It will illustrate important differences between the adaption of the HPDLM and GDLM to a structural break,
% and it will show how, the HPDLM can approximate an intervention without any \textit{ex ante} input from the analyst.

% I compare the performance of the horseshoe prior innovations model ($M_{nile,HS}$) with two alternative models.
% The first model has a uses a normal distribution with a time-invariant variance for the innovations ($M_{nile,normal}$).
% The second model extends $M_{nile,normal}$ to include a single parameter that represents change in the level after 1899 ($M_{nile,normal2}$).
% The details of these models is given in Section \ref{sec:nile-1}.
% Figure \ref{fig:nile} plots the original data, and the mean of the posterior predictive distributions for each of these models.
% The  shows a sharp drop 1899 and stability before and after the break.
% The normal model $M_{nile,normal}$ shows a smother adjustment with the decline in the level beginning a few years before 1899 and continuing a few years thereafter.
% Model $M_{nile,normal}$ also shows more variability in the level before and after 1899.
% This variability illustrates the importance of using a scale mixtures of normal distributions with local shrinkage parameters ($\lambda_{t}$) in addition to a global shrinkage parameter ($\tau$).
% Since the normal model has only a single global shrinkage parameter ($\tau$). 
% In order to accommodate the large change in the level in 1899, the estimated value of $\tau$ must increase.
% However, increasing $\tau$ will result in less smoothing in the other observations.
% The normal model must trade off shrinking the non-structural breaks and not shrinking the structural break with only a single parameter, resulting in over-smoothing around the break and under-smoothing elsewhere.

% It is also remarkable that the horseshoe prior model's posterior predictive means closely match those of the intervention model ($M_{nile,normal2}$) without any \textit{ex ante} knowledge of the presence of the structural break in 1899.
% There is a slight difference in the two models in that the horseshoe prior model puts some weight on the possibility that the structural break occurred in 1897 or 1898.
% This is most likely due to the low signal to noise ratio in the data; note that the observations in 1897 and 1898 are consistent with, although high for, the distribution of flows after 1899.
% The Nile model is an easy case in that the series has a single, large level change with a clear causal event, and thus easy to include a dummy variable.
% However, in many applications, the presence of the structural break will not be known, and in fact estimating the presence and location of the structural breaks will be the purpose of the application.

% Figures \ref{fig:nile_innovations} and \ref{fig:nile_w} show the results of the two methods that could be used to identify structural breaks. 
% Figure \ref{fig:nile_innovations} plots the mean and 95 percent HPD interval of each $p(\omega_{t} | y)$.%
% \footnote{For $M_{nile,normal2}$, the posterior distribution $p(\omega_{t} + \delta (x_{t} - x_{t-1}) | y)$ is used.}
% The normal model $M_{nile,nomral}$ shows no structural breaks, while the intervention model $M_{nile,normal2}$ shows a clear structural break at 1899.
% In the horseshoe model, the estimated mean of $p(\omega_{1899} | y)$ is large, suggesting a structural break, although its 95 percent credible interval does not cross zero.
% As noted before, this seems to be due to small, although highly variable estimates, of $\omega$ in 1897 and 1898, suggesting small probabilities that the structural break occurred in those years.
% Note that the observed data in those years is consistent with the upper tail of the distribution after 1899.
% The reason that there is near certainty of a structural break in 1899 as opposed to the two earlier years is due to outside data, the knowledge that the dam was built in that year.
% However, the second method, using the values of $w_{t}$ classifies 1899 as a structural break, giving the $\Pr(\omega_{1899} \neq 0) \approx 0.55$.

% \begin{figure}[htpb]
%   \centering
%   \includegraphics{plots/fig-nile.pdf}
%   \caption{Plot of mean posterior predictive distributions ($\E p(\tilde{y}| y)$) for the normal, normal2, and horseshoe prior distribution models.}
%   \label{fig:nile}
% \end{figure}

% \begin{figure}[htpb]
%   \centering
%   \includegraphics{plots/fig-nile_w.pdf}
%   \caption{Plot of estimated probability of a structural break, calculated as $w_{i} = 1 - \E(\hat{\kappa})$}
%   \label{fig:nile_w}
% \end{figure}

% \begin{figure}[htpb]
%   \centering
%   \includegraphics{plots/fig-nile_innovations.pdf}
%   \caption{Plot of innovations}
%   \label{fig:nile_innovations}
% \end{figure}

% \begin{table}[htpb]
%   \centering
%   \input{plots/tab-nile.tex}
%   \caption{Model summary statistics of Nile models.}
%   \label{tab:nile}
% \end{table}

\section{Implementation}
\label{sec:implementation}

% There are a wide variety of sofware that can estimate state space models.
% See the special issue of the \textit{Journal of Statistical Software} provides an overview of the software available to estimate state space models, including in R, Stata, and Matlab.%
% \footnote{Also see \textcite{Tusell2011} for a review of R packages for Kalman filtering and smoothing.}

Although state space models are often estimated using Bayesian MCMC methods, methods to efficiently sapmle from DLMs are not supported by most general purpose Bayesian software (OpenBUGS, JAGS, and PyMC).
While, the  equations of the dynamic linear model (~\eqref{eq:8} and \eqref{eq:14}) can be easily translated into the BUGS language, it is difficult to efficiently estimate the models.
That is because, BUGS and JAGS treat each $\alpha_{t}$ as a seperate parameter, and sample $\alpha_{t}$ conditional on the values of the other $\alpha_{-t}$.
Due to the high correlation between $\alpha_{t}$ and $\alpha_{t+1}$, this sampling procedure will generally result in poor mixing \parencite[477]{Jackman2009}.

To avoid this poor mixing, more efficient, specialized methods to sample from joint distribution of $\alpha_{1:n}$ have been developed.
A standard method for sampling from the joing distribution of $\alpha$ is the Forward-Filter Backward Sampler (FFBS), independently developed by \textcite{CarterKohn1994} and \textcite{Fruehwirth-Schnatter1994}.
Subsequently, more efficient sampling methods have been developed by \textcite{DeJongShephard1995}, \textcite{DurbinKoopman2002}, \textcite{StricklandTurnerDenhamEtAl2009}, and \textcite{ChanJeliazkov2009}.
However, the methods for sampling from the joing distribution of $\alpha$ are not included in the general purpose software (OpenBUGS, JAGS, PyMC), and thus a researcher interested in estimating dynamic linear models has had to write a custom samplers for each application.

This paper shows a method for easily and efficiently estimating the posterior distribution of the state space model using Stan and R.
This paper shows how Stan can be used to easily and efficiently estimate the posterior distribution of these models.
Like BUGS, Stan \textcite{Stan2013}{Stan2013a} is a general purpose Bayesian software with a domain specific language that hides the details of the sampling from the user.
Howeer, rather than using Gibbs Sampling like BUGS and JAGS, Stan implements a variant of Hamiltonian Monte Carlo (HMC) algorithm, the details of which are in \textcite{HoffmanGelman2013}.

The posterior distribution of a dynamic linear model can be sampled from using Stan and R.
This method proceeds in two steps.
\begin{enumerate}
\item First, the system matrices ($T$, $Z$, $H$, $Q$, $R$), or rather any parameters in them are sampled within Stan. This can be done efficiently within Stan without sampling from $\alpha_{1:n}$ by adding the likelihood $p(y_{1:n} | T_{1:n}, Z_{1:n}, Q_{1:n}, R_{1:n}, H_{1:n})$ to the log-posterior of the model.
The likelihood $p(y_{1:n} | T_{1:n}, Z_{1:n}, Q_{1:n}, R_{1:n}, H_{1:n})$ can be efficiently calculated using the Kalman filter.
\item Second, to sample from $\alpha$, for each sample of the posterior distribution of $T_{1:n}, Z_{1:n}, Q_{1:n}, R_{1:n}, H_{1:n}$, draw a sample of $\alpha$ using FFBS or a simulation smoother. In this paper, I use the R package \textbf{KFAS}.
\end{enumerate}

A useful feature of Stan's programming language is that custom distributions can be implemented by simply adding their contribution to the log posterior distribution.
In the case of a dynamic linear model, this means that to estimate parameters in the system matrices  $p(y_{1:n} | T_{1:n}, Z_{1:n}, Q_{1:n}, R_{1:n}, H_{1:n})$.
The likelihood of the DLM can be efficiently calculated using the Kalman filter. 
And the Kalman filter can be written in the Stan modeling language.

\subsubsection{Other Examples}

This paper needs some / good examples.

\begin{itemize}
\item Presidential approval for George W. Bush. \parencites{RatkovicEng2010}
\item Median ideal point of the Senate. \parencites{RatkovicEng2010}
\item Supreme court dissents and concurrences. 1 or 2 structural breaks. Poisson data. \parencite{CalderiaZorn1998}
\item Discrete DV change-point models in \parencite{spirling2007bayesian}.
\item Interest Rates, Inflation, and GDP growth are common economics examples, e.g. \textcite{GiordaniKohn2008}.
\end{itemize}

\clearpage{}
\section{Appendix}
\label{sec:appendix}

\subsection{Models}
\label{sec:models}

\subsubsection{Nile}
\label{sec:nile-1}

Model $M_{nile,normal}$ is a local level model with a normal distribution.
The observation variance is given an improper Jeffrey's prior.
The system variance (global scale parameter) is given a half-Cauchy distribution. 
The initial state is given a semi-informative prior, a normal distribution with a mean at the value of the first observation, and variance equal to the sample variance of the data.
\begin{equation}
  \label{eq:11}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\theta_{t}, \sigma^{2}} \\
    \theta_{t} &\sim \dnorm{\theta_{t - 1}, \sigma^{2} \tau^{2}} \\
    p(\sigma^{2}) &= \frac{1}{\sigma^{2}} \\
    \tau &\sim \dhalfcauchy{0, 1} \\
    p(\theta_{1}) &\sim \dnorm{y_{1}, \Var{y}}
  \end{aligned}
\end{equation}

Model $M_{nile,HS}$ differs from $M_{nile,normal}$ in that it assumes a horseshoe prior distribution on 
the innovations,
\begin{equation}
  \label{eq:18}
  \begin{aligned}[t]
    \theta_{t} &\sim \dnorm{\theta_{t - 1}, \sigma^{2} \lambda_{t}^{2} \tau^{2}} \\
    \lambda &\sim \dhalfcauchy{0, 1} \\
    \tau &\sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}

Model $M_{nile,HS}$ differs from $M_{nile,normal}$ by adding an intervention parameter $\delta$ to the observation equation to model the level shift. 
The data $x_{t}$ is a binary vector equal to 0 before 1899, and 1 thereafter.
\begin{equation}
  \label{eq:19}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\theta_{t} + \delta x_{t}, \sigma^{2}} \\
    \delta &\sim \dunif{-\infty, \infty}
  \end{aligned}
\end{equation}

\clearpage{}

\printbibliography{}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

%  LocalWords:  Carvallho
