\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{fancyvrb}
% \usepackage{color}
\usepackage[latin1]{inputenc}
\usepackage[style=authoryear]{biblatex}
\usepackage{graphicx}
\usepackage{subcaption}
\addbibresource{default}
%\addbibresource{local}
\usepackage{setspace}
\doublespace

\usepackage{todo}

\author{Jeffrey B. Arnold}
% Seeing a Shrink about Structural Breaks
\title{Sparse State Disturbance Dynamic Linear Models for Structural Breaks}


% Used to typeset distributions
\newcommand{\dist}[1]{\mathcal{#1}}
\newcommand{\paren}[1]{\ensuremath{\left(#1\right)}}
\newcommand{\dnorm}[1]{\ensuremath{\dist{N}\paren{#1}}}
\newcommand{\dmvnorm}[2]{\ensuremath{\dist{N}_{#2}\paren{#1}}}
\newcommand{\dt}[2]{\ensuremath{\dist{T}_{#1}\paren{#2}}}
\newcommand{\dcauchy}[1]{\ensuremath{\dist{C}\paren{#1}}}
\newcommand{\dhalfcauchy}[1]{\ensuremath{\dist{C}^{+}\paren{#1}}}
\newcommand{\dbeta}[1]{\ensuremath{\dist{B}\paren{#1}}}
\newcommand{\dinvbeta}[1]{\ensuremath{\dist{IB}\paren{#1}}}
\newcommand{\dgamma}[1]{\ensuremath{\dist{G}\paren{#1}}}
\newcommand{\dinvgamma}[1]{\ensuremath{\dist{IG}\paren{#1}}}
\newcommand{\dwishart}[1]{\ensuremath{\dist{W}\paren{#1}}}
\newcommand{\dinvwishart}[1]{\ensuremath{\dist{IW}\paren{#1}}}
\newcommand{\dunif}[1]{\ensuremath{\dist{U}\paren{#1}}}

\newcommand{\RLang}{\textsf{R}}
\newcommand{\Stan}{Stan}
\newcommand{\R}{\ensuremath{\mathbb{R}}} %real

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\Var}{Var}
\DeclareMathOperator{\Cov}{Cov}
\DeclareMathOperator{\diag}{diag}
\newcommand{\tran}{^\top}

\include{pygments}
\begin{document}

\maketitle{}


\section{Introduction}
\label{sec:introduction}

Political and social processes are rarely, if ever, constant over time.
Thus political scientists often have a need to estimate time-varying parameters (TVP).
There exist two broad approaches to estimating time-varying parameters: structural break approaches, including dummy variables and change point models, and smoothing approaches, including dynamic linear models and smoothing splines.
These two approaches are viewed as distinct and estimated using different methods, forcing the researcher to choose between the models.
Since in many processes the changes in the process are characterized by many periods of stability and a few periods of possibly rapid and large, change \parencite{RatkovicEng2010},
which 
structural break (change point) models are often used \parencites{CalderiaZorn1998}{Spirling2007}{Spirling2007a}{Park2010}{Park2011}.%
\footnote{\textcite{RatkovicEng2010} is the notable exception in that their method combines both smoothly varying sections with structural breaks.}
However, many in general the structural break methods become more difficult to formulate and estimate as the number of breaks goes from known to estimated.

This paper presents a simple and flexible method to estimate time-varying parameters that may be subject to structural breaks.
Time-varying parameters with possible structural breaks can be estimated within a continuous state-space (dynamic linear model) model by placing a shrinkage prior on the distribution of the state disturbances.
The intuition behind this can be illustrated with a simple model.
Suppose there is a vector of observed data, $y_{1}, \dots, y_{n}$, drawn from a normal distribution with a time varying mean, $\alpha_{1:n}$. 
This can be represented within a state space model as follows,
\begin{equation}
  \label{eq:4}
  \begin{aligned}[t]
    y_{t} &= \alpha_{t} + \varepsilon_{t} & \varepsilon_{t} \sim N(0, \sigma^{2}) \\
    \alpha_{t + 1} &= \alpha_{t} + \eta_{t}
  \end{aligned}
\end{equation}
The difference between "structural breaks" and "smoothing" data-generating processes and estimation techniques is whether the $\eta$ vector is assumed (estimated) to be sparse (most $\eta_{t} = 0$) or dense (most $\eta_{t} \neq 0$).
The commonly estimated local level dynamic linear model specifies a common normal distribution for $\eta$.
While this can estimate change over time, it cannot capture sparse $\eta$, as it will tend to oversmooth the breaks and undersmooth the periods of stability.
However, estimating sparse parameter vectors is a general problem that has received considerable attention lately in large-p, small-n problems (include citations).
This paper applies some of those advances to estiamting time-varying parameters, such as \eqref{eq:4}.
Instead of using a prior normal distribution on $\eta$, a shrinkage prior is used instead.
While there are a large number of Bayesian shrinkage priors proposed, this paper will use the Horseshoe Prior distribution introduced in \textcites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}.

Using a sparse distrurbance representation of TVP models has multiple favorable characteristics.
\begin{enumerate}
\item This method does not require specifying the number of structural breaks ex ante.
Structural breaks can be detected from $\eta$ using several rules.
The sparsity of $\eta$ will determine the number of structural breaks, and this sparsity can be 
Not only do the number of structural breaks need not be specified beforehand, this method will work reasonable well even if the underlying data-generating process has the parameter changing in each period.
\item This method is flexible.
Dynamic linear models incorporate a wide variety of models, including ARIMA and structural time-series, cubic splines, and regressions with time-varying coefficients.
Any model in which the parameter of interest can be expressed as a latent state in a dynamic linear model can be altered to assign a shrinkage prior to the state disturbance in order to make it robust to or to detect structural breaks for that parameter.
\item The method allows for easy estimation of structural breaks in multiple parameters which can be either independent or correlated.
\item This method can be altered to detect outliers, simply by applying shrinkage priors to the observation disturbances instead of the state disturbances in the dynamic linear model.
\item This method is efficient in both programmer and computational time, while still retaining the flexibility to estimate a wide variety of models.
Since many most shrinkage priors, including the Horseshoe Prior used in the paper, are scale-mixture of normal distributions, this method can take advantage of the computationally efficient methods of mode finding and sampling from dynamic linear models, such as the Kalman filter and Forward-Filter Backwards-Sample.
This paper shows how a combination of \Stan, a general purpose Bayesian software prgram, and \RLang can be used to easily estimate and sample from the posterior of dynamic linear models.
\end{enumerate}


\section{Dynamic Linear Models}
\label{sec:dynam-line-models}

A Dynamic Linear Model (DLM), also called a Linear Gaussian state space model, for a $n$-dimensional observation sequence $y_{1}, \dots, y_{n}$ is defined by the following set of equations.%
\footnote{This paper follows the notation used in \textcite{DurbinKoopman2001}. See \textcite{PetrisPetroneEtAl2009} for a concordance with the notation used in \textcite{WestHarrison1997}.}
For $t = 1:n$,
\begin{align}
  \label{eq:8}
  \underset{p \times 1}{y_t} &= \underset{p \times m}{Z_{t}} \, \underset{m \times 1}{\alpha_t} + \underset{p \times 1}{\varepsilon_t} & \varepsilon_{t} &\sim \dmvnorm{0, H_{t}}{p} \\
  \label{eq:14}
  \underset{m \times 1}{\alpha_{t+1}} &= \underset{m \times m}{T_{t}} \, \underset{m \times 1}{\alpha_{t}} + \underset{m \times r}{R_{t}}  \underset{r \times 1}{\eta_{t}} & \eta_{t} &\sim \dmvnorm{0, Q_{t}}{r} \\
  \label{eq:2}
  \alpha_{1} & \sim \dmvnorm{a_{1}, P_{1}}{m}
\end{align}

Equation \eqref{eq:8} is the \textit{observation equation} which relates the \textit{observation vector} $y_{t}$ to the \textit{state vector} $\alpha_{t}$.
Equation \eqref{eq:14} is the \textit{state equation} which describes the Markovian evolution of the state vector.
Equation \eqref{eq:2} is the \textit{initial state equation} which is a prior distribution for the initial state $\alpha_{1}$.
The vectors $\varepsilon_{t}$ and $\eta_{t}$ are referred to as the \textit{observation} and \textit{state disturbances}, respectively.
The matrices $Z_{t}$, $H_{t}$, $T_{t}$, $R_{t}$, and $Q_{t}$ are referred to as the \textit{system matrices}.
Let $\mathcal{S}_{t}$ refer to the set of system matrices.
The matrix $Z_{t}$ is the design matrix, $T_{t}$ is the transition matrix, $H_{t}$ is the observation covariance matrix, and $R_{t} Q_{t} R'_{t}$ is the state covariance matrix.
For the purposes of the estimation of $\alpha$, they are considered fixed and known, but in a larger model they can include parameters to be estimated.
%Let $p$ be the dimension of the observation vector (number of variables), $m$ be the dimension of the state vector, and $r$ be the dimension of the state disturbances.
%then the dimensions of the elements in the DLM are show in Table \ref{tab:state_space_dim}.
% \begin{table}[!]
%   \centering
% \begin{tabular}{llll}
%   Vectors & dimension & Matrices & dimension \\
% \hline
%   $y_t$     & $p, 1$ & $Z_{t}$  & $p, m$ \\
%   $\alpha_{t}$ & $m, 1$ & $H_{t}$  & $p, p$ \\
%   $\varepsilon_t$ & $p, 1$ & $T_{t}$ & $m, m$ \\
%   $\eta$ & $r, 1$ & $R_t$ & $m, r$ \\
%    &  & $Q_t$ & $r, r$ \\
%   $a_{1}$ & $m, 1$ & $P_{1}$ & $m, m$
% \end{tabular}
%   \caption{The imensions of the elements in DLM (Equations \eqref{eq:8}, \eqref{eq:14}, and \eqref{eq:2})}
%   \label{tab:state_space_dim}
% \end{table}

Many common models, including ARIMA and structural time-series, regressions with time-varying coefficients, cubic splines, and stochastic volatility models can be represented as DLMs. 
See \parencites{WestHarrison1997}{DurbinKoopman2001}{PetrisPetroneEtAl2009}{CommandeurKoopman2007}.%
\footnote{See \textcite{CommandeurKoopmanOoms2011} for a review of statistical software to estimate state space models.}
This flexibility is important because the methods presented below can be applied to a wide range of models.
Any model in which the parameter of interest can be represented as a state in a DLM, can be estimated using the following method to detect structural breaks.


\subsection{Structural Breaks in a State Space Model}
\label{sec:struct-breaks-state}

For simplicity of exposition, in this section I will restric my attention to a univariate observation vector $y_{1:n}$ with a time-varying mean.
This can be represented as a dynamic linear model as follows,
\begin{equation}
  \label{eq:5}
  \begin{aligned}[t]
    y_{t} &= \alpha_{t} + \varepsilon_{t} & \varepsilon & \sim \dnorm{0, H_{t}} \\
    \alpha_{t + 1} &= \alpha_{t} + \eta_{t} & \eta & \sim \dnorm{0, Q_{t}}
  \end{aligned}
\end{equation}
Equation \eqref{eq:5} is simply a DLM with $p = m = r = 1$, and $Z_{t} = T_{t} = 1$.
The model in Equation \eqref{eq:5} is commonly called the \textit{local level model} or a \textit{random walk with noise}.

In equation \eqref{eq:5}, changes in the state are equal to the value of the state disturbance, $\eta_{t} = \alpha_{t+1} - \alpha_{t}$. 
Thus, the distribution of parameter changes is simply the distribution of $\eta_{t}$.
In many problems in political and social science, the evolution of a parameter is expected to to be stable for long periods of time, with a few changes of possibly large magnitude, i.e. the structural breaks.
Translated into the \eqref{eq:5}, this means that the researcher expects that the vector $\eta$ is sparse. 
For most periods $\eta_{t} = 0$, but there are a few periods in which $\eta_{t} \neq 0$, and, possibly,  $|\eta_{t}| \gg 0$.
When recast in this way, the problem of estimating structural breaks within a dynamic linear model is essentially a problem of estimating a sparse parameter vector $\eta$.
Estimating sparse parameters, especially when the number of observations is less than the number of parameters (the large-p, small-n situtation) is a problem that has received and is currently receiving much attention.
Following the convention in that literature, I will refer $\eta_{t} \approx 0$ as ``noise'' and $|\eta_{t}| \gg 0$ as ``signals''.
In sparse estimation problems, the researcher wants to classify signals from noise in the parameters, which in structural break applications is to classify structural breaks (signals) and non-structural breaks (noise).

In Bayesian estimation, there are two main approaches for estimating sparse parameters: discrete mixtures and shrinkage priors.
The first approach models each parameter with a prior consisting of a discrete mixture of a point mass at zero and a continuous distribution.
This mixture distribution is a commonly called a spike-and-slab prior.
This approach includes variable selection priors and Bayesian model selection and averaging.
In the context of estimating TVP, structural breaks can be estimated if $\eta$ has a spike-and-slab prior distribution,
\begin{equation}
  \label{eq:1}
  \eta_{t} = p \delta_{0} +  (1 - p) g(\eta_{t})
\end{equation}
where $p \in [0, 1]$, $\delta_{0}$ is the point mass distribution at 0, and $g(\eta_{t})$ is the distribution if $\eta \neq 0$.
\textcite{GiordaniKohn2008} propose using \eqref{eq:1} as a flexible model of structural breaks within a continuous state space framework.

The second approach to estimating sparse parameters is shrinkage priors. 
Shrinkage priors are absolutely continuous distributions centered at zero.
Although shrinkage priors do not mix between signal and noise groups, they are able to approximate the mixture distribution if they have the following features,
\begin{itemize}
\item a large mass near zero to shrink noise observations
\item heavy tails to keep signals unshrunk.
\end{itemize}
Shrinkage priors are the Bayesian posterior estimation equivalent of penalized likelihood for posterior mode-finding.
For example, the popular Lasso/L1 regularization corresponds to \textit{maximum a posteriori} (MAP) estimation with a Laplacian (double exponential) prior on the parameters.
Other prior distributions correspond to other, sometimes implicit and lacking analytic form, likelihood penalties.

Almost all of these shrinkage priors can be represented as scale mixtures of normal distributions \parencite{PolsonScott2010}.
The scale mixtures are generally of the product of a global variance component $\tau^{2}$ and a local variance component $\lambda^{2}$,
\begin{equation}
  \label{eq:3}
  \begin{aligned}[t]
    \eta_{t} &= N(0, \tau^{2} \lambda_{t}^{2})
    \lambda_{t}^{2} &\sim p(\lambda_{t}^{2})
  \end{aligned}
\end{equation}
Shrinkage priors differ in the distribution of the local variance component.%
Global shrinkage via $\tau$ handles shrinks noise, while local values of $\lambda_{t}$ act to detect the signals.\parencite[5]{PolsonScott2010}
\footnote{
Note that the commonly used $\eta_{t} \sim N(0, Q)$ is trivially a scale-mixture of normals ($\lambda_{t}^{2} = \delta_{1}$).
However, it does not impose sparsity. 
The normal distribution does not have a large mass at zero, and thus undershrinks noise, and it has very thin tails, and thus overshrinks signals.
}
That $\eta$ is distributed as a scale mixture of normal distributions is important is important because that means altough $\eta$ is not distributed normal, conditional on $\lambda_{t}$, $\eta_{t}$ is distributed normal, and thus the entire system is a Conditional Gaussian DLM.
This means that conditional on the values of $\lambda$, the DLM can still be estimated using the efficient methods developed for DLMs.

While there are an increasing number of proposed shrinkage priors, this paper will use the Horseshoe Prior distribution \parencites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}{PolsonScott2010}{PolsonScott2012}{DattaGhosh2012}.
The Horseshoe Prior distribution has no analytical form, but can be represented as a scale-mixture of normal distributions (as in \eqref{eq:3}) in which the local scale components $\lambda$ are distributed half-Cauchy,
\begin{equation}
  \label{eq:6}
  \lambda \sim \dhalfcauchy{0, 1}
\end{equation}
or equivalently, the local variance components are distributed inverse-beta,
\begin{equation}
  \label{eq:9}
  \lambda^{2} \sim \dinvbeta{\frac{1}{2}, \frac{1}{2}}
\end{equation}
The Horseshoe prior distribution has many appealing properties.
It has both an infinitely high spike at zero, and Cauchy-like tails.
This allows it to aggressively shrink noise towards zero, while leaving signals unshrunk.
Figure \ref{fig:horseshoe} compares the density function of the Horseshoe Prior to the normal, Cauchy, and Laplacian (double-exponential) distributions, both around zero and in the tails.
\begin{figure}
  \centering
  % \includegraphics{plots/fig-horseshoe1.pdf}
  % \includegraphics{plots/fig-horseshoe2.pdf}
  \caption{The density of the horseshoe prior distribution (in black) compared with the densities of the normal, Cauchy, and Laplacian distributions (in gray).}
  \label{fig:horseshoe}
\end{figure}

Using shrinkage priors to model structural breaks extends and generalizes the use of the $t$-distribution for modeling structural breaks \textcite{HarveyKoopman2000}[184][]{DurbinKoopman2001}{PetrisPetroneEtAl2009}.
While the $t$-distribution has heavy tails, it does not have a substantial mass near zero, and thus will not shrink noise disturbances.
Many more recently discovered sparse prior distributions, including the Horseshoe Prior, have better sparsity properties than the $t$-distribution.

To summarize, the proposed local level model that is robust to structural breaks is as follows,
\begin{equation}
  \label{eq:10}
  \begin{aligned}[t]
    y_{t} &\sim N(\alpha_{t}, H) \\
    \alpha_{t + 1} &\sim N(\alpha_{t}, \tau^{2} \lambda^{2}_{t}) \\
    \lambda^{2}_{t} & \sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}
This model could be completed with the noninformative priors,
\begin{equation}
  \label{eq:7}
  \begin{aligned}[t]
    p(H) &= \frac{1}{H} \\
    \tau &\sim \dhalfcauchy{0, H}
  \end{aligned}
\end{equation}
$H$ is given its Jeffrey's prior, while $\tau$ is distributed Cauchy
on the same scale as $H$.


\subsection{Identifying Structural Breaks}

Structural breaks can be detected from the estimates of the  posterior distribution in a couple of ways.
The first method is simply to check whether the credible interval of the posterior distribution of the state disturbances $p(\eta_{t} | y, .)$ crosses zero.
This is the Bayesian equivalent of the auxiliary residual test in \textcite{JongPenzer1998}{DurbinKoopman2001}.%
Apart from other differences between credible and confidence intervals, the Bayesian estimate has the advantage of marginalizing over the posterior distribution of the system matrix parameters and thus accounting for the  uncertainty in the estimates of the other parameters.
The maximum likelihood method simply evaluates the auxiliary residuals at the mode of the other parameters.%
\footnote{Depending on the application it may be more appropriate to directly analyze $p(\alpha_{t} - \alpha_{t-1})$, which also includes the linear transformations in the system matrices $Z_{t}$ and $c_{t}$ which may include parameters.}
However, it may be better to calculate one sided probability statements, such as $\Pr(\eta_{t} | y > 0)$ or $\Pr(\eta_{t} | y <0)$.

The second method, suggested by \textcite[179-180]{PetrisPetroneEtAl2009} is to use $\lambda_{t}$ identify structural breaks.
If $\lambda_{t} = 1$, then the state disturbance is distributed normal with global scale $\eta_{t} \sim N(0, \tau)$.
Thus, the local shrinkage values $\lambda_{t}$ are a roughly measure of the shrinkage of $\eta_{t}$.
Values of $\lambda_{t} < 1$ are observations that are shrunk
Values of $\lambda_{t} > 1$ put a higher prior probability on larger absolute values of $\lambda_{t}$.

Alternatively, the researcher can choose not to try to classify the periods as structural breaks or not, but rather directly focus on the magnitudes of $\eta_{t}$.
For example, it may be of more practical interest to order the $\eta_{t}$ by their absolute values.


\subsection{Multivariate States}
\label{sec:multivariate}

Sparse priors distributions on the state disturbances can be applied to multivariate distributions.
If the state disturbances are independent, then each state disturbance can be given its own shrinkage prior.
However, often the state disturbances can be correlated. In that case, scale mixture of normal distributions can be extended to the multivariate case as follows,%
\footnote{
  This extension seems intuitive, but I cannot find any one who has done this.
  Am I doing something crazy or stupid here? 
}
\begin{align}
  \label{eq:12}
  \eta_{t} \sim \dmvnorm{0, \Lambda_{t} \Gamma \Lambda_{t}'}{m} \\
  \label{eq:13}
  \Lambda \Lambda' & \sim p(\Lambda \Lambda')
\end{align}
where $\Gamma$ is the global covariance component, and $\Lambda_{t}$ is a lower triangular matrix such that  $\Lambda_{t} \Lambda_{t}'$ is the local covariance matrix.
I generalize the Horseshoe Prior distribution to matrix variates by decomposing the local covariance matrix into a standard deviation vector and a correlation matrix,
\begin{align}
  \label{eq:16}
  \Lambda_{t} \Lambda_{t}' &= \diag(\lambda_{t}) R_{t} \diag(\lambda_{t})' \\
  \label{eq:17}
  \lambda_{t,i} &= \dhalfcauchy{0, 1} & \text{for $i \in 1:p$}
\end{align}
where $R_{t}$ is a correlation matrix.
Each element in $\lambda_{.,i}$ for all $t \in 1:n$ share a common prior distribution, shrinking the values across time.


\subsection{Outliers}
\label{sec:outliers}

This paper is primarily concerned with structural breaks, but scale-mixture of normal distribution shrinkage priors can also be used to model outliers simply
by placing shrinkage priors on the observation disturbances, $\varepsilon$.
A major difference between observation disturbances and state disturbances, is that there is often more concern about sparsity in the state disturbances than in the observation disturbances.
For example, many statistical models are estimated with non-time varying parameters, but none are estimated without any observation variance.
The primary concern with outliers is not sparsity, but the possibility of $\varepsilon \gg 0$.
Thus, the $t$-distribution, which has fat tails but not a spike at zero, will likely work well in many applications.
The use of the $t$-distribution to model outliers has a long history in Bayesian models generally (CITE), and has been suggested for DLMs (CITE).


\section{Examples}
\label{sec:examples}


\subsection{Nile Flow Data}
\label{sec:nile}

The first example is the Nile river flow data, which is a classic dataset in the  \parencites{Cobb1978}{Balke1993}{JongPenzer1998}{DurbinKoopman2001}{DurbinKoopman2012}
The data consist of annual observations of the flow of the Nile river at Ashwan between 1871 and 1970.
It is well known that there was a level shift in 1899, both due to the construction of a damn at Ashwan and weather changes.
Since in this example, there is only one clear change point, it is not fully exploiting the flexibility of this method, but is instead a sanity check.
It will illustrate important differences between the adaption of the HPDLM and GDLM to a structural break,
and it will show how, the HPDLM can approximate an intervention without any \textit{ex ante} input from the analyst.

I compare the performance of the horseshoe prior innovations model ($M_{nile,HS}$) with two alternative models.
The first model has a uses a normal distribution with a time-invariant variance for the innovations ($M_{nile,normal}$).
The second model extends $M_{nile,normal}$ to include a single parameter that represents change in the level after 1899 ($M_{nile,normal2}$).
The details of these models is given in Section \ref{sec:nile-1}.
Figure \ref{fig:nile} plots the original data, and the mean of the posterior predictive distributions for each of these models.
The  shows a sharp drop 1899 and stability before and after the break.
The normal model $M_{nile,normal}$ shows a smother adjustment with the decline in the level beginning a few years before 1899 and continuing a few years thereafter.
Model $M_{nile,normal}$ also shows more variability in the level before and after 1899.
This variability illustrates the importance of using a scale mixtures of normal distributions with local shrinkage parameters ($\lambda_{t}$) in addition to a global shrinkage parameter ($\tau$).
Since the normal model has only a single global shrinkage parameter ($\tau$). 
In order to accommodate the large change in the level in 1899, the estimated value of $\tau$ must increase.
However, increasing $\tau$ will result in less smoothing in the other observations.
The normal model must trade off shrinking the non-structural breaks and not shrinking the structural break with only a single parameter, resulting in over-smoothing around the break and under-smoothing elsewhere.

It is also remarkable that the horseshoe prior model's posterior predictive means closely match those of the intervention model ($M_{nile,normal2}$) without any \textit{ex ante} knowledge of the presence of the structural break in 1899.
There is a slight difference in the two models in that the horseshoe prior model puts some weight on the possibility that the structural break occurred in 1897 or 1898.
This is most likely due to the low signal to noise ratio in the data; note that the observations in 1897 and 1898 are consistent with, although high for, the distribution of flows after 1899.
The Nile model is an easy case in that the series has a single, large level change with a clear causal event, and thus easy to include a dummy variable.
However, in many applications, the presence of the structural break will not be known, and in fact estimating the presence and location of the structural breaks will be the purpose of the application.

Figures \ref{fig:nile_innovations} and \ref{fig:nile_w} show the results of the two methods that could be used to identify structural breaks. 
Figure \ref{fig:nile_innovations} plots the mean and 95 percent HPD interval of each $p(\omega_{t} | y)$.%
\footnote{For $M_{nile,normal2}$, the posterior distribution $p(\omega_{t} + \delta (x_{t} - x_{t-1}) | y)$ is used.}
The normal model $M_{nile,nomral}$ shows no structural breaks, while the intervention model $M_{nile,normal2}$ shows a clear structural break at 1899.
In the horseshoe model, the estimated mean of $p(\omega_{1899} | y)$ is large, suggesting a structural break, although its 95 percent credible interval does not cross zero.
As noted before, this seems to be due to small, although highly variable estimates, of $\omega$ in 1897 and 1898, suggesting small probabilities that the structural break occurred in those years.
Note that the observed data in those years is consistent with the upper tail of the distribution after 1899.
The reason that there is near certainty of a structural break in 1899 as opposed to the two earlier years is due to outside data, the knowledge that the dam was built in that year.
However, the second method, using the values of $w_{t}$ classifies 1899 as a structural break, giving the $\Pr(\omega_{1899} \neq 0) \approx 0.55$.

% \begin{figure}[htpb]
%   \centering
%   \includegraphics{plots/fig-nile.pdf}
%   \caption{Plot of mean posterior predictive distributions ($\E p(\tilde{y}| y)$) for the normal, normal2, and horseshoe prior distribution models.}
%   \label{fig:nile}
% \end{figure}

% \begin{figure}[htpb]
%   \centering
%   \includegraphics{plots/fig-nile_innovations.pdf}
%   \caption{Plot of innovations}
%   \label{fig:nile_innovations}
% \end{figure}

% \begin{table}[htpb]
%   \centering
%   \input{plots/tab-nile.tex}
%   \caption{Model summary statistics of Nile models.}
%   \label{tab:nile}
% \end{table}

\section{Implementation}
\label{sec:implementation}

MCMC sampling from a dynamic linear model is challenging due to the temporal dependence of the latent state parameters $\lambda_{t}$ \parencite{ReisSalazarGamerman2006}.
Although full conditional distributions for a Gibbs sampler can be easily specified (Carlin et al 1992), in practice sampling component-wise will almost always result in slow convergence and highly correlated posterior samples.
However, in the case of dynamic linear models there exist more efficient block sampling algorithms which sample from $\alpha_{t}$ in a single block.
These methods include the Forward-Filter Backward Smoothing algorithm of \textcite{CarterKohn1994} and \textcite{Fruehwirth-Schnatter1994}, as well as more efficient simulation smoothers of \textcite{DeJongShephard1995}, \textcite{DurbinKoopman2002}, \textcite{StricklandTurnerDenhamEtAl2009}.%
\footnote{Also see \textcite{ReisSalazarGamerman2006} for a comparison of the efficiency of various sampling methods, and \textcite{migon2005dynamic} for an overview of the various sampling methods.}
Additionally, since in the case of a Gaussian dynamic linear models the posterior $p(\alpha | y)$ is multivariate normal with a sparse, block diagonal covariance matrix, samples can be drawn directly from the posterior distribution \parencites{migon2005dynamic}{ChanJeliazkov2009}).

However, the efficient algorithms are not included in the commonly used general-purpose Bayesian software programs (BUGS, JAGS, and PyMc).
Thus, to estimate and sample from dynamic linear models in a computationally efficient manner, the researcher must write a custom MCMC sampler.
While there exists many software implementaitons of filters and smoothers for Dynamic Linear Models, sampling from other parameters may require additional sampling tricks due to the correlation between the states and the parameters (Weis, and citations, Chan Jeliazkov).

To estimate the models in this paper, I used a combination of Stan and R.

Stan is a general purpose Bayesian software program.
Like BUGS, it has a domain specific language that lets the user specify the statistical model without specifying the steps used to sample from the model.
Unlike BUGS, Stan is not based on Gibbs sampling, but instead uses a variant of Hamiltonian Monte Carlo (citation).
While as of the time of the writing, a distribution for dynamic linear models is not included in the software, but a method for efficiently sampling the other parameters while margininalizing over the latent states can be implemented within the Stan modeling language.
The objective in the Stan step is draw posterior samples from (any parameters in) the state matrices $S_{t}$.
Think of dynamic linear model as  $p(y | S_{t})$. 
To sample from a distribution not built into Stan, all that is required is the computation of the log likelihood of that distribution, which is added to the log-posterior.
In the case of dynamic linear models, the log-likelihood of $p(y | S_{t})$ can be efficiently calculated with the Kalman filter (citations).

Thus, to get a sample from the
\begin{enumerate}
\item Sample $k$ iterations from the posterior distriubtion of $p(S_{t} | y_{t})$ by using a Kalman filter implemented in Stan to calculate $\log p(y_{t} | S_{t})$.
\item For each sample from the posterior distribution of $p(S_{t} | y_{t})$, draw $\alpha | y_{t}, S_{t}$ using an efficient sampling algorithm.
This paper used the R package KFAS (cite).
\end{enumerate}

Separating the steps in this manner allows for efficient sampling of parameters in $S$ with HMC, while marginalizing over $\alpha$.
If samples from $\alpha$ are needed, then they can be sampled after, possibly in parallel.

\subsubsection{Other Examples}

This paper needs some / good examples.

\begin{itemize}
\item Presidential approval for George W. Bush. \parencites{RatkovicEng2010}
\item Median ideal point of the Senate. \parencites{RatkovicEng2010}
\item Supreme court dissents and concurrences. 1 or 2 structural breaks. Poisson data. \parencite{CalderiaZorn1998}
\item Discrete DV change-point models in \parencite{spirling2007bayesian}.
\item Interest Rates, Inflation, and GDP growth are common economics examples, e.g. \textcite{GiordaniKohn2008}.
\end{itemize}

\section{Conclusion}
\label{sec:conclusion}

This paper shows that a simple tweak to dynamic linear models allow them to estimat structural break models with a random number of breaks.
The sparse disturbance approach is both intuitive and flexible, while remaining computationally efficient.

\clearpage{}
\section{Appendix}
\label{sec:appendix}

\subsection{Models}
\label{sec:models}

\subsubsection{Nile}
\label{sec:nile-1}

Model $M_{nile,normal}$ is a local level model with a normal distribution.
The observation variance is given an improper Jeffrey's prior.
The system variance (global scale parameter) is given a half-Cauchy distribution. 
The initial state is given a semi-informative prior, a normal distribution with a mean at the value of the first observation, and variance equal to the sample variance of the data.
\begin{equation}
  \label{eq:11}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\theta_{t}, \sigma^{2}} \\
    \theta_{t} &\sim \dnorm{\theta_{t - 1}, \sigma^{2} \tau^{2}} \\
    p(\sigma^{2}) &= \frac{1}{\sigma^{2}} \\
    \tau &\sim \dhalfcauchy{0, 1} \\
    p(\theta_{1}) &\sim \dnorm{y_{1}, \Var{y}}
  \end{aligned}
\end{equation}

Model $M_{nile,HS}$ differs from $M_{nile,normal}$ in that it assumes a horseshoe prior distribution on 
the innovations,
\begin{equation}
  \label{eq:18}
  \begin{aligned}[t]
    \theta_{t} &\sim \dnorm{\theta_{t - 1}, \sigma^{2} \lambda_{t}^{2} \tau^{2}} \\
    \lambda &\sim \dhalfcauchy{0, 1} \\
    \tau &\sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}

Model $M_{nile,HS}$ differs from $M_{nile,normal}$ by adding an intervention parameter $\delta$ to the observation equation to model the level shift. 
The data $x_{t}$ is a binary vector equal to 0 before 1899, and 1 thereafter.
\begin{equation}
  \label{eq:19}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\theta_{t} + \delta x_{t}, \sigma^{2}} \\
    \delta &\sim \dunif{-\infty, \infty}
  \end{aligned}
\end{equation}

\clearpage{}

\printbibliography{}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

%  LocalWords:  Carvallho
