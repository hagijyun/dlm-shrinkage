\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}
\usepackage{fancyvrb}
% \usepackage{color}
\usepackage[latin1]{inputenc}
\usepackage[style=authoryear]{biblatex}
\usepackage{graphicx}
\usepackage{subcaption}
\addbibresource{local}
\usepackage{setspace}
\doublespace

\author{Jeffrey B. Arnold}
% Seeing a Shrink about Structural Breaks
\title{Scale Mixture of Gaussians Dynamic Linear Models for Sparse 
  Time-Varying Parameter Change}


\newcommand{\paren}[1]{\ensuremath{\left(#1\right)}}
\newcommand{\dnorm}[1]{\ensuremath{\mathcal{N}\paren{#1}}}
\newcommand{\dmvnorm}[2]{\ensuremath{\mathcal{N}_{#2}\paren{#1}}}
\newcommand{\dt}[2]{\ensuremath{\mathcal{T}_{#1}\paren{#2}}}
\newcommand{\dcauchy}[1]{\ensuremath{\mathcal{C}\paren{#1}}}
\newcommand{\dhalfcauchy}[1]{\ensuremath{\mathcal{C}^{+}\paren{#1}}}
\newcommand{\dbeta}[1]{\ensuremath{\mathcal{B}\paren{#1}}}
\newcommand{\dinvbeta}[1]{\ensuremath{\mathcal{IB}\paren{#1}}}
\newcommand{\dgamma}[1]{\ensuremath{\mathcal{G}\paren{#1}}}
\newcommand{\dinvgamma}[1]{\ensuremath{\mathcal{IG}\paren{#1}}}
\newcommand{\dwishart}[1]{\ensuremath{\mathcal{W}\paren{#1}}}
\newcommand{\dinvwishart}[1]{\ensuremath{\mathcal{IW}\paren{#1}}}
\newcommand{\dunif}[1]{\ensuremath{\mathcal{U}\paren{#1}}}

\newcommand{\RLang}{\textsf{R}}
\newcommand{\R}{\ensuremath{\mathbb{R}}} %real

\DeclareMathOperator{\E}{E}
\DeclareMathOperator{\var}{Var}
\DeclareMathOperator{\cov}{Cov}

\include{pygments}
\begin{document}

\maketitle{}

\begin{abstract}
  In estimating problems with time-varying parameters, researchers often have to choose between methods that smooth the change over time and methods that model the change as discrete breaks.
  This paper proposes using dynamic linear models with scale-mixture of gaussians a model time-varying processes that can account for either or both smoothly time-varying processes and processes with large discrete jumps.
  The problem of estimating time-varying parameters is a special case of the ``large-p'' problem, and this paper applies recent advances in that literature to the time-varying parameter problem.
  This provides a robust and flexible method. 
\end{abstract}

\section{Introduction}
\label{sec:introduction}

Political processes are rarely constant over time, so there often a need for political science researchers to estimate time-varying parameters.
Suppose there are $T$ time periods, and the researcher would like to estimate a parameter $\theta$ that can take a different values in each period, $\theta_{t}$ for $t \in 1:T$.
Often there is only a single data point per time period, making the problem of estimating time varying parameters a small-n, large-p problem. 
Thus, some sort of additional structure needs to be imposed on multivariate distribution of $\theta$ in order to estimate the time varying parameters.%
Since the parameters are ordered by time, it is common to structure the differences in $\theta$, $\theta_{t} - \theta_{t - 1}$.
\footnote{The alternative approach is to ignore the ordering of $\theta$ and apply a prior distribution on the $\theta$, as in random effects.}
There are two classes of models for time-varying parameters.
The first class smooths parameter changes over time, by shrinking the values of $\Delta \theta_{t}$.
Examples of this approach are dynamic linear models, polynomial, and splines.
The second class selects times, usually a small number, at which there are non-zero changes in the parameter and the size of the non-zero changes.
This is equivalent to setting the function of the parameter values with respect to time to be a step function.
This approach includes structural break, regime-shift, change-point of the locations of the breaks is unknown, and indictator variables if the location of the breaks is known.

Modeling time-varying parameters with structural breaks with scale-mixtures of normal distributions is an approach that has favorable properties in terms of its flexibility, ease of implementation, and computational efficiency.
\begin{itemize}
\item Unlike many existing discrete state-space approaches, the number of change points (or equivalently the sparsity in the changes in the parameters) does not need to be specified \textit{ex ante}, but can be estimated.
\item This method is flexible.
  This is a special case of a conditional gaussian dynamic linear models (CGDLM) \parencites{WestHarrison1997}{DurbinKoopman2001}{CommandeurKoopman2007}{ShumwayStoffer2010}.
  DLMs are a class of models that include regression, stochastic volatility, and ARIMA models.
  The structural break method presented here can easily be adapted to model changes in multiple parameters, seasonal effects, slopes, and variance.
\item It is convenient to independently model changes in multiple parameters, a situation which would cause an explosion in the number of states in discrete state space framework (Hidden Markov).
\item Since the horseshoe prior distribution is hierarchical distribution formed from normal and Cauchy distributions, this model can easily be specified in general purpose Bayesian software (BUGS, Stan, etc.), even though that is not the most efficient method of estimating it.
\end{itemize}

\section{State Space Models}

A Dynamic Linear Model (DLM) is defined by the set of equations,
\begin{align}
  \label{eq:8}
  Y_t &= F_{t} \theta_t + \nu_t & \nu_{t} &\sim \dmvnorm{0, V_{t}}{m} \\
  \label{eq:14}
  \theta_t &= G_{t} \theta_{t-1} + \omega_{t} & \omega_{t} &\sim \dmvnorm{0, W_{t}}{p} \\
  \label{eq:2}
  \theta_{0} & \sim \dmvnorm{m_{0}, C_{0}}{p}
\end{align}  
where $t = 1:T$, 
$Y_{t}$ and $\nu_{t}$ are length $m$ vectors,
$\theta_{t}$, $m_{0}$, and $\omega_{t}$ are length $p$ vectors,
$F_{t}$ is a $m \times p$ matrix, 
$V_{t}$ is an $m \times m$ matrix,
and $G_{t}$, $C_{0}$ and $W_{t}$ are $p \times p$ matrices.
Equation \eqref{eq:8} is called the \textit{observation} or \textit{measurement} equation, 
equation \eqref{eq:14} is called the \textit{system equation},
and \eqref{eq:4} is the prior distribution on the initial state.
I will refer to the $\nu_{t}$ as \textit{errors}, and the $\omega_{t}$ as \textit{innovations}.
DLMs nest a large number of common models, including ARIMA, stochastic volatility, (time-varying parameter) regressions,
and cubic splines \parencites{WestHarrison1997}{DurbinKoopman2001}{ShumwayStoffer2010}{CommandeurKoopman2007}.%
\footnote{See \textcite{CommandeurKoopmanOoms2011} for a review of statistical software to estimate state space models.}

For the purpose of motivating the estimation of time-varying parameters in this section, I will consider a special case of DLMs, a univariate local level model.
The univariate local level model assumes a univariate $Y_{t} = y_{t}$, $F_{t} = 1$, and $G_{t} = 1$.
\begin{align}
  \label{eq:15}
  y_t &= \theta_t + \nu_t & \nu_{t} &\sim \dnorm{0, v_{t}} \\
  \label{eq:16}
  \theta_t &= \theta_{t-1} + \omega_{t} & \omega_{t} &\sim \dnorm{0, w_{t}} \\
\end{align}
Note that in this model, given the initial value $\theta_{0}$, the values of $\theta$ are determined by the distribution of the $\omega$, which is a distribution over the changes in $\theta$,
\begin{equation}
  \label{eq:12}
  \Delta \theta_{t} = \theta_{t} - \theta_{t - 1} = \omega_{t} \sim \dnorm{0, w_{t}} \text{.}
\end{equation}
Thus, the choice of the distributions of the $\omega$ parameters determines how the parameter $\theta$ can evolve over time.%
\footnote{Although I only consider Bayesian framework, the choice of prior distribution is equivalent to a choice of a penalty term in maximum likelihood framework \parencite{PolsonScott2010}.}

A common approach is to assume that the innovations have a constant variance over time, $w_{t} = w$  for all $t \in 1:T$.
If $\omega$ is distributed normal, it penalizes large changes in $\omega$ due to the thin tails of the normal distribution.%
\footnote{This is equivalent to an L2 penalty.}
This effectively smooths the evolution of $\theta$ over time.

The problem with assuming $\omega_{t} \sim \dnorm{0, w}$ is that it does not incorporate handle sparsity and large values well, two features which are expected in many data generating processes encounted in political science.

\subsection{Discrete Mixture Distribution}
\label{sec:discr-mixt-distr}

The first approach are selection approaches, in which the estimation technique selects which $\omega_{t}$ are non-zero (usually a small number), and then estimates the values of the non-zero innovations.
Models within this approach are usually formulated and estimated as discrete state-space Hidden Markov Models, \parencites{Chib1998}{spirling2007bayesian}{Park2011}{Park2010}{Blackwell2012}.
A downside of these methods is that they often require the number of discrete states to be set \parencite{Chib1998}, and although methods exist that relax that assumption, they are often not straighforward.

However, structural breaks can also be estimated with continuouous state space approach \parencite{GiordaniKohn2008}.
The key feature of structural break approaches is sparsity in changes in the parameter, i.e. most changes are 0, with a few non-zero changes.
The sparsity of changes in $\theta$ can be modeled with a spike and slab prior distribution on $\omega$,
i.e. $\omega$ is given a discrete mixture between the a degenerate distribution at zero and another continuous distribution.
\begin{equation}
  \label{eq:1}
  \omega_{t} \sim p g(\omega_{t}) + (1 - p) \delta_{0} \text{,}
\end{equation}
where $p$ is the prior probability of a structural break (change-point), and $g(\omega_{t})$ is the distribution of the change in $\theta$ if there is a structural break.
Alternatively, \eqref{eq:1} can be written as,
\begin{equation}
  \label{eq:7}
  \begin{aligned}[t]
    \omega_{t} & \sim \lambda_{t} \eta_{t} \\
    \eta_{t} & \sim \dnorm{0, \tau} \\
    \lambda_{t} = & 
    \begin{cases}
      1 & \text{with probability $p$} \\
      0 & \text{with probability $1 - p$}
    \end{cases}
  \end{aligned}
\end{equation}

This model of structural breaks has an advantage over discrete state space models in that the number ofstructural breaks does not need to be specified \textit{ex ante} by the researcher.
The value of $p$ determines the prior sparsity of changes (and thus the number of structural breaks), and can either be specified by the researcher or given a prior distribution and estimated from the data.

The problem with using a spike and slab approach is that it is implausible in applied practice that the actual changes in the parameter are exactly zero, violating the assumptions of the spike and slab prior distribution \parencite{Gelman2013}.
Also practically, the posterior distribution from a spike-and-slab prior will not put a point mass on zero.
So it may be easier to estimate a continuous distribution that 

In practice, the structural break (sparse) approaches to time varying parameters are used for two reasons.
First, even if the actual changes in political processes is not zero, political processes are marked by periods of relative stability and periods of rapid change.
Traditionally, there have not been continuous distributions that could adapt to rapid change, e.g. the normal distribution smooths of those rapid changes.
Second, sparse results have advantages in intepretability.
Structural breaks can be understood by their proximity to real-world events, whereas continous change is harder to make sense of.

The next section discusses the use of continuous distributions (shrinkage priors) that are robust to sparse changes.
This draws on many recent advances in the Bayesian shrinkage and regression regularization literature.

\subsection{Shrinkage}
\label{sec:shrinkage}

The shrinkage approach in estimating time-varying approach is to penalize large values of $\omega_{t}$. 
The predominant example of this approach is the normal dynamic linear model, in which the $\omega_{t}$ are distributed i.i.d. normal,
\begin{equation}
  \label{eq:4}
  \omega_{t} \sim \dnorm{0, \tau^{2}}
\end{equation}
Since the normal distribution has thin tails, it penalizes large values of $\omega_{t}$ and thus smooths the values of $\theta_{t}$ over time.
\footnote{The normal dynamic linear model is similar to ridge regression for the innovations.}
This approach works well when the value of $\theta$ changes slowly over time.
However, many political processes are marked by periods of stability and points of rapid change \parencite{RatkovicEng2010}.
A data generating process with structural breaks poses problems for the normal dynamic linear model.
In order to accomodate large structural breaks, the posterior estimate of $\tau$ must increase. 
This results in undersmoothing (overfitting) in periods of relative stability, while still oversmoothing (underfitting) structural breaks.

However, as noted before, the problem of estimating $\omega$ is an example of a large-p problem and there are many proposed distributions for shrinkage parameters.
The class of scale-normal mixtures includes many Bayesian shrinkage priors, such as the student-\textit{t} \parencite{Tipping2001}, double-exponential prior (Bayesian LASSO) \parencites{LiGoel2006}{ParkCasella2008}{Hans2009}, normal-Jeffreys \parencites{FigueiredoMember2003}{BaeMallick2004}, Strawderman-Berger \parencites{Strawderman1971}{Berger1980}, double Pareto \parencite{ArmaganDunsonLee2011},  and normal-exponential-gamma \parencite{BrownGriffin2005}, normal/gamma and normal/inverse-gamma \parencite{CaronDoucet2008}{BrownGriffin2010}.
Since many computationally efficient forms of maximization and sampling of the dynamic linear model require the errors and innovations be distributed normal, I will focus on a class of shrinkage distributions that are scale mixtures of normal distributions, i.e. each $\omega_{t}$ will be distributed normal, but the variances of these normal distributions are drawn from a hierarchical distribution.
\begin{equation}
  \label{eq:6}
  \begin{aligned}[t]
    \omega_{t} | \tau^{2}, \lambda^{2} & \sim \dnorm{0, \sigma^{2} \lambda^{2}} \\
    \lambda_{t}^{2} & \sim p(\lambda^{2}_{t})
  \end{aligned}
\end{equation}
where $\tau^{2}$ is called the global shrinkage parameter, and $\lambda_{t}^{2}$ are called the local shrinkage parameters.
The $t$-distribution is an example of a scale mixture of normal distributions, and has been suggested for dynamic linear model estimation that is robust to structural breaks \parencites{HarveyKoopman2000}{PetrisPetroneEtAl2009}.
The $t$-distribution in its most extreme form (Cauchy), has very flat tails, which allows for structural breaks.
However, it does not have a large spike at zero, and thus may not shrink noise enough.

The distribution of $\omega$ which will be used in this paper is the horseshoe prior distribution, introduced in \textcites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}.
The horseshoe prior distribution does not have an analytical form, but is formed when the $\lambda_{t}$ in equation \eqref{eq:6} are independently distributed half-Cauchy,
\begin{align}
  \label{eq:13}
  \lambda_{t} &\sim \dhalfcauchy{0, 1}
\end{align}
where $\dhalfcauchy{0, \gamma}$ is the standard half-Cauchy distribution with support on the positive real numbers, and scale $\gamma$.%
\footnote{
  This implies that $p(\lambda^{2})$ is distributed inverse-beta, $IB(a, b)$ where $a = b = \frac{1}{2}$ \parencite[4]{PolsonScott2010}. 
}

The horseshoe prior distribution has two features that make it useful as a shrinkage prior for sparse parameters.
It has flat Cauchy-like tails, which mean that structural breaks are not shrunk \textit{a posteriori}.
It also has an infinitely tall spike at zero which aggressively shrinks non-structural breaks to zero.
Figure \ref{fig:horseshoe} compares the density of the horseshoe prior distribution against the normal, Cauchy, and Laplacian (Baysian LASSO) distributions.
The Laplacian distribution is commonly used in sparse regularized regression.
However, the horseshoe prior distribution has both a taller spike at zero and fatter tails than the Laplacian distribution.

\begin{figure}
  \centering
  \includegraphics{plots/fig-horseshoe1.pdf}
  \includegraphics{plots/fig-horseshoe2.pdf}
  \caption{The density of the horseshoe prior distribution (in black) compared with the densities of the normal, Cauchy, and Laplacian distributions (in gray).}
  \label{fig:horseshoe}
\end{figure}


To summarize, a local level with a horseshoe prior on the innovations,
\begin{equation}
  \label{eq:3}
  \begin{aligned}[t]
    y_{t} & \sim \dnorm{\theta_{t}, \sigma^{2}} \\
    \theta_{t} & \sim \dnorm{\theta_{t - 1}, \sigma^{2} \tau^{2} \lambda^{2}} \\
    \lambda_{t} & \sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}
The values $\sigma$ and $\tau$ can be estimated as parameters. 
In which case, the following prior distributions are suggested in \textcite{CarvalhoPolsonScott2010}{DattaGhosh2012},
\begin{align}
  \label{eq:9}
  \begin{aligned}[t]
    p(\sigma^{2}) & \frac{1}{\sigma^{2}}  \\
    \tau &\sim \dhalfcauchy{0, 1} \text{.}
  \end{aligned}
\end{align}

\subsection{Identifying Structural Breaks}
\label{sec:ident-struct-breaks}

The posterior distribution $p(\theta | y)$ can be used to identify structural breaks in two ways. 

The first method is simply to use the posterior distribution of $\omega_{t} = \theta_{t} - \theta_{t-1}$.
The probability that there was a positive change in $\theta$ at time $t$ is
$\Pr(\omega_{t} | y) > 0$, and the probability of a negative change is $\Pr(\omega_{t} | y) < 0$.
Alternatively, the highest posterior density region of the posterior can be calculated and the researcher can classify observations as structural breaks if the credible interval for a given probability does not cross zero.

The second method is to calculate a quantity similar to the posterior probability $\omega \neq 0$ that would be produced by a spike and slab distribution.
For the discrete mixture model in equation \eqref{eq:1}, if $g$ is sufficiently heavy-tailed, the posterior mean,
\begin{equation}
  \label{eq:17}
  \E(\omega_{t} | y) \approx p_{t} e_{t} 
\end{equation}
where $p_{t}$ is the posterior probability of $\omega_{t} \neq 0$.
For a scale mixture of normal distributions, the expected value of $\omega_{t}$ is 
\begin{equation}
  \label{eq:10}
  E(\omega_{t} | y) =
  \left(
    1 - \frac{1}{1 + \lambda^{2}_{t} \tau^{2}}
  \right) e_{t} = (1 - \hat \kappa_{t}) e_{i}
\end{equation}
Equating equation \eqref{eq:17} with equation \eqref{eq:10}, it is clear that 
the quantity $1 - \hat \kappa_{t}$ behaves similarly to $p_{t}$.
While $1 - \hat \kappa_{t}$ can be calculated for all mixture of normal distributions, it is not necessarily the case that $1 - \hat \kappa_{t} \approx p_{t}$.
However, one of the advantages of using the Horseshoe Prior distribution is that 
\parencite[474]{CarvalhoPolsonScott2010} show through simulations that this is the case for the Horseshoe Prior distribution.
Given that, \textcite{CarvalhoPolsonScott2010} recommend the following  decision rule under a 0-1 loss function to classify whether an observation is a signal (structural break in this case) or noise (no structural break), 
\begin{equation}
  \label{eq:5}
  \text{$H_{0,t}$ if $\nu_{t} = 1 - E(\kappa_{t}|y_{t}, \nu_{t-1} \lambda_{t}, \tau, \sigma) > \frac{1}{2}$}
\end{equation}

\section{Monte Carlo}
\label{sec:monte-carlo}

This paper probably needs a Monte Carlo.

\section{Examples}
\label{sec:examples}

\subsection{Nile Flow Data}
\label{sec:nile}

The first example is the Nile river flow data, which is a classic dataset in the  \parencites{Cobb1978}{Balke1993}{JongPenzer1998}{DurbinKoopman2001}{DurbinKoopman2012}
The data consist of annual observations of the flow of the Nile river at Ashwan between 1871 and 1970.
It is well known that there was a level shift in 1899, both due to the construction of a damn at Ashwan and weather changes.
Since in this example, there is only one clear change point, it is not fully exploiting the flexibility of this method, but is instead a sanity check.
It will illustrate important differences between the adaption of the HPDLM and GDLM to a structural break,
and it will show how, the HPDLM can approximate an intervention without any \textit{ex ante} input from the analyst.

I compare the performance of the horseshoe prior innovations model ($M_{nile,HS}$) with two alternative models.
The first model has a uses a normal distribution with a time-invariant variance for the innovations ($M_{nile,normal}$).
The second model extends $M_{nile,normal}$ to include a single parameter that represents change in the level after 1899 ($M_{nile,normal2}$).
The details of these models is given in Section \ref{sec:nile-1}.
Figure \ref{fig:nile} plots the original data, and the mean of the posterior predictive distributions for each of these models.
The  shows a sharp drop 1899 and stability before and after the break.
The normal model $M_{nile,normal}$ shows a smother adjustment with the decline in the level beginning a few years before 1899 and continuing a few years thereafter.
Model $M_{nile,normal}$ also shows more variability in the level before and after 1899.
This variability illustrates the importance of using a scale mixtures of normal distributions with local shrinkage parameters ($\lambda_{t}$) in addition to a global shrinkage parameter ($\tau$).
Since the normal model has only a single global shrinkage parameter ($\tau$). 
In order to accommodate the large change in the level in 1899, the estimated value of $\tau$ must increase.
However, increasing $\tau$ will result in less smoothing in the other observations.
The normal model must trade off shrinking the non-structural breaks and not shrinking the structural break with only a single parameter, resulting in over-smoothing around the break and under-smoothing elsewhere.

It is also remarkable that the horseshoe prior model's posterior predictive means closely match those of the intervention model ($M_{nile,normal2}$) without any \textit{ex ante} knowledge of the presence of the structural break in 1899.
There is a slight difference in the two models in that the horseshoe prior model puts some weight on the possibility that the structural break occurred in 1897 or 1898.
This is most likely due to the low signal to noise ratio in the data; note that the observations in 1897 and 1898 are consistent with, although high for, the distribution of flows after 1899.
The Nile model is an easy case in that the series has a single, large level change with a clear causal event, and thus easy to include a dummy variable.
However, in many applications, the presence of the structural break will not be known, and in fact estimating the presence and location of the structural breaks will be the purpose of the application.

Figures \ref{fig:nile_innovations} and \ref{fig:nile_w} show the results of the two methods that could be used to identify structural breaks. 
Figure \ref{fig:nile_innovations} plots the mean and 95 percent HPD interval of each $p(\omega_{t} | y)$.%
\footnote{For $M_{nile,normal2}$, the posterior distribution $p(\omega_{t} + \delta (x_{t} - x_{t-1}) | y)$ is used.}
The normal model $M_{nile,nomral}$ shows no structural breaks, while the intervention model $M_{nile,normal2}$ shows a clear structural break at 1899.
In the horseshoe model, the estimated mean of $p(\omega_{1899} | y)$ is large, suggesting a structural break, although its 95 percent credible interval does not cross zero.
As noted before, this seems to be due to small, although highly variable estimates, of $\omega$ in 1897 and 1898.
However, the second method, using the values of $w_{t}$ classifies 1899 as a structural break, giving the $\Pr(\omega_{1899} \neq 0) \approx 0.55$.

\begin{figure}[htpb]
  \centering
  \includegraphics{plots/fig-nile.pdf}
  \caption{Plot of mean posterior predictive distributions ($\E p(\tilde{y}| y)$) for the normal, normal2, and horseshoe prior distribution models.}
  \label{fig:nile}
\end{figure}

\begin{figure}[htpb]
  \centering
  \includegraphics{plots/fig-nile_w.pdf}
  \caption{Plot of estimated probability of a structural break, calculated as $w_{i} = 1 - \E(\hat{\kappa})$}
  \label{fig:nile_w}
\end{figure}

\begin{figure}[htpb]
  \centering
  \includegraphics{plots/fig-nile_innovations.pdf}
  \caption{Plot of innovations}
  \label{fig:nile_innovations}
\end{figure}

\begin{table}[htpb]
  \centering
  \input{plots/tab-nile.tex}
  \caption{Model summary statistics of Nile models.}
  \label{tab:nile}
\end{table}

\clearpage{}

\subsubsection{Other Examples}

This paper needs a political science example.

\begin{itemize}
\item Presidential approval for George W. Bush. \parencites{RatkovicEng2010}
\item Median ideal point of the Senate. \parencites{RatkovicEng2010}
\item Supreme court dissents and concurrences. 1 or 2 structural breaks. Poisson data. \parencite{CalderiaZorn1998}
\item Discrete DV change-point models in \parencite{spirling2007bayesian}.
\item Inflation or GDP?
\end{itemize}

\section{Appendix}
\label{sec:appendix}


\subsection{Models}
\label{sec:models}

\subsubsection{Nile}
\label{sec:nile-1}

Model $M_{nile,normal}$ is a local level model with a normal distribution.
The observation variance is given an improper Jeffrey's prior.
The system variance (global scale parameter) is given a half-Cauchy distribution. 
The initial state is given a semi-informative prior, a normal distribution with a mean at the value of the first observation, and variance equal to the sample variance of the data.
\begin{equation}
  \label{eq:11}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\theta_{t}, \sigma^{2}} \\
    \theta_{t} &\sim \dnorm{\theta_{t - 1}, \sigma^{2} \tau^{2}} \\
    p(\sigma^{2}) &= \frac{1}{\sigma^{2}} \\
    \tau &\sim \dhalfcauchy{0, 1} \\
    p(\theta_{1}) &\sim \dnorm{y_{1}, \var{y}}
  \end{aligned}
\end{equation}

Model $M_{nile,HS}$ differs from $M_{nile,normal}$ in that it assumes a horseshoe prior distribution on 
the innovations,
\begin{equation}
  \label{eq:18}
  \begin{aligned}[t]
    \theta_{t} &\sim \dnorm{\theta_{t - 1}, \sigma^{2} \lambda_{t}^{2} \tau^{2}} \\
    \lambda &\sim \dhalfcauchy{0, 1} \\
    \tau &\sim \dhalfcauchy{0, 1}
  \end{aligned}
\end{equation}

Model $M_{nile,HS}$ differs from $M_{nile,normal}$ by adding an intervention parameter $\delta$ to the observation equation to model the level shift. 
The data $x_{t}$ is a binary vector equal to 0 before 1899, and 1 thereafter.
\begin{equation}
  \label{eq:18}
  \begin{aligned}[t]
    y_{t} &\sim \dnorm{\theta_{t} + \delta x_{t}, \sigma^{2}} \\
    \delta &\sim \dunif{-\infty, \infty}
  \end{aligned}
\end{equation}


\subsubsection{CP6}
\label{sec:cp6}


% \subsection{Code}
% \label{sec:code}

% \subsection{Normal DLM}

% \input{plots/code-normal.tex}


% \subsection{Horseshoe Prior DLM}

% \input{plots/code-horseshoe.tex}

\printbibliography{}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

%  LocalWords:  Carvallho
