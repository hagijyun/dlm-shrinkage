\documentclass{article}

\usepackage{amsmath}
\usepackage{amsfonts}
\usepackage[margin=1in]{geometry}

\usepackage[style=authoryear]{biblatex}

\usepackage{graphicx}
\usepackage{subcaption}

\bibliography{local}

\usepackage{setspace}
\doublespace

\author{Jeffrey B. Arnold}
\title{Scale Mixture of Normal Dynamic Linear Models: \\
  A Unified Model of Time-Varying Parameters and Change-Points}

\begin{document}

\maketitle{}

\section{Introduction}
\label{sec:introduction}

When estimating time-varying parameters, there exist two primary approaches.
The first approach is change-point or structural break models. 
In these models, the change in the parameter only occur at a few points and is contant otherwise.
In other words, the parameter is modeled with step function.
The second approach is time-varying parameter models in which the parameter changes in all periods.
This approach includes dynamic linear models and smoothing splines.

However, both structural break and time-varying parameters model can be modeled with dynamic linear models.
The difference between these approaches is in the distributions of the \textit{innovations}, the changes in the parameter values.
In structural break models, the innovations have a spike and slab distribution, which is a discrete mixture between 0 and another, usually diffuse, distribution.
In time-varying parameter models, the innovations are given a continuous distribution, usually the normal.

This paper proposes using scale mixture of normal distributions for the innovation distributions in order to nest both structural break and time varying parameter models.
The insight is that the problem posed in estimating time-varying parameters shares many characteristics with the ``large-p'' literature.
In the case of the a univariate time-series with $n$ obserservations and a time-varying mean, the number of innovation parameters to be estimated is $n - 1$.

\begin{itemize}
\item This paper does not require an \textit{ex ante} specification of
  the number of structural breaks. 
  The number of structural breaks can be estimated from the data.
  And unlike some structural break methods which estimate the number of change points, is is comparatively straight forward.
\item This method nests both smoothly time-varying parameters and
  structural break models.There are several reasons to use continuous
  distributions to model time-varying parameters possibly with
  structural breaks.  In the case of truly sparse data, discrete
  mixtures are computationally inefficient.  And in the case of
  non-sparse change-points, the discrete mixture is unnecessary.
\item This method is flexible.  This is a special case of a normal
  dynamic linear models
  (DLM) \parencites{WestHarrison1997}{DurbinKoopman2012}{CommandeurKoopman2007}{ShumwayStoffer2010}.
  DLMs are flexible class of models that include linear regressions
  and ARIMA models.
\item Moreover for the case in which $\epsilon_{t}$ and $\eta_{t}$ are distributed normal, there are computationally efficient methods both to maximize the likelihood of (Kalman Filter) and to sample from (Forward-filter backward sample) \eqref{eq:8}.
\end{itemize}

\section{Dynamic Linear Model}

Consider a univariate vector of data $y$, indexed by $t = 1:T$.
Each $y_{t}$ is drawn from a normal distribution with a time-varying mean, $\alpha_{t}$.
\begin{align}
  \label{eq:8}
  y_t &= \alpha_t + \epsilon_t \\
  \label{eq:14}
  \alpha_t &= \alpha_{t-1} + \eta_{t} & \text{for $t > 1$} \\
  \label{eq:19}
  \alpha_1 &= \alpha_{1} + \eta_{1}
\end{align}
where $\epsilon_{t} \sim N(0, \sigma^{2})$ and $E(\eta_{t}) = 0$.
The model described by equations \eqref{eq:8}, \eqref{eq:14}, and \eqref{eq:19} is 
is a special case of dynamic linear models called the local level model.
Equation \eqref{eq:8} is called the observation equation, 
equation \eqref{eq:14} is called the system equation,
and \eqref{eq:4} is the prior distribution.
The parameters $\alpha_{t}$ are the latent states, which in this case are the means of $y_{t}$.
The parameters $\eta_{t}$ are called the \textit{innovations}.

The inference problem in \eqref{eq:8} is to estimate the values of $\alpha_{t}$ for all time periods.
These values of $\alpha_{t}$ are determined by the initial values $\alpha_{1}$ and the sum of the innovations $\sum_{s=1}^{t} \eta_{s}$.
Thus, estimating $\alpha_{t}$ is equivalent to the problem of estimating $\eta_{t}$.
Since the number of $\eta$ parameters is equal to the number of data points ($y$), this problem can be seen as a case of the ``large-p'' problem, and some sort of dimension reduction or regularization is needed to estimate the values of and $\eta$, and thus $\alpha$.

Assumptions about this regularization through the choice of the prior distribution of $\eta$.
There are two features for which any prior of $\eta$ must account.
First, $\eta_{t}$ is likely sparse; in most time periods, the change in $\alpha$ is (close-to) zero.
Second, $\eta_{t}$ may be large; there are occasionally quite large jumps in $\alpha$.
In other words, processes (at least those encountered in political science) are likely to be stable or slowly varying for most periods, but with occasionally dramatic changes at key events \parencite{RatkovicEng2010}.

In the Bayesian literature, there are two main approaches to esimating time-varying parameters: discrete mixtures (Hidden Markov Models for change-points), and shrinkage (dynamic linear models).
The approach in this paper is to use a shrinkage approach with scale-mixtures of normal distributions that are able to handle both sparsity and jumps.

The first approach are selection approaches, in which the estimation technique selects which $\eta_{t}$ are non-zero (usually a small number), and then estimates the values of the non-zero innovations.
Models within this approach are usually formulated and estimated as discrete state-space Hidden Markov Models, as in Chib 1998 and extensions thereof (Spirling Park, Koop).
However, this approach can also be represented within the continuous state-space approach by giving $\eta_{t}$ spike-and-slab mixture distributions \parencite{GiordaniKohn2008},
\begin{equation}
  \label{eq:1}
  \eta_{t} \sim p g(\eta_{t}) + (1 - p) \delta_{0} \text{,}
\end{equation}
where $p$ is the prior probability of a structural break (change-point), and $g(\eta_{t})$ is the distribution of the change in $\alpha$ if there is a structural break.

The shrinkage approach in estimating time-varying approach is to penalize large values of $\eta_{t}$. 
The predominant example of this approach is the normal dynamic linear model, in which the $\eta_{t}$ are distributed i.i.d. normal,
\begin{equation}
  \label{eq:4}
  \eta_{t} \sim N(0, \tau^{2})
\end{equation}
Since the normal distribution has thin tails, it penalizes large values of $\eta_{t}$ and thus smooths the values of $\alpha_{t}$ over time.
\footnote{The normal dynamic linear model is similar to ridge regression for the innovations.}
This approach works well when the value of $\alpha$ changes slowly over time.
However, many political processes are marked by periods of stability and points of rapid change \parencite{RatkovicEng2010}.
A data generating process with structural breaks poses problems for the normal dynamic linear model.
In order to accomodate large structural breaks, the posterior estimate of $\tau$ must increase. 
This results in undersmoothing (overfitting) in periods of relative stability, while still oversmoothing (underfitting) structural breaks.

However, as noted before, the problem of estimating $\eta$ is an example of a large-p problem and there are many proposed distributions for shrinkage parameters.
The class of scale-normal mixtures includes many Bayesian shrinkage priors, such as the student-\textit{t} \parencite{Tipping2001}, double-exponential prior (Bayesian LASSO) \parencites{LiGoel2006}{ParkCasella2008}{Hans2009}, normal-Jeffreys \parencites{FigueiredoMember2003}{BaeMallick2004}, Strawderman-Berger \parencites{Strawderman1971}{Berger1980}, double Pareto \parencite{ArmaganDunsonLee2011},  and normal-exponential-gamma \parencite{BrownGriffin2005}, normal/gamma and normal/inverse-gamma \parencite{CaronDoucet2008}{BrownGriffin2010}.
Since many computationally efficient forms of maximization and sampling of the dynamic linear model require the errors and innovations be distributed normal, I will focus on a class of shrinkage distributions that are scale mixtures of normal distributions, i.e. each $\eta_{t}$ will be distributed normal, but the variances of these normal distribtions are drawn from a hiearchical distribution.
\begin{equation}
  \label{eq:6}
  \begin{aligned}[t]
    \eta_{t} | \tau^{2}, \lambda^{2} & \sim N(0, \tau^{2} \lambda_{t}^{2}) \\
    \lambda_{t}^{2} & \sim p(\lambda^{2}_{t})
  \end{aligned}
\end{equation}
where $\tau^{2}$ is called the global shrinkage parameter, and $\lambda_{t}^{2}$ are called the local shrinkage parameters.
The $t$-distribution is an example of a scale mixture of normal distributions, and has been suggested for dynamic linear model estimation that is robust to structural breaks \parencites{HarveyKoopman2000}{PetrisPetroneEtAl2009}.
The $t$-distribution in its most extreme form (Cauchy), has very flat tails, which allows for structural breaks.
However, it does not have a large spike at zero, and thus may not shrink noise enough.

The distribution of $\eta$ which will be used in this paper is the horseshoe prior distribution, introduced in \textcites{CarvalhoPolsonScott2009}{CarvalhoPolsonScott2010}.
The horseshoe prior distribution does not have an analytical form, but is formed when the $\lambda_{t}$ in equation \eqref{eq:6} are independently distributed half-Cauchy,
\begin{align}
  \label{eq:13}
  \lambda_{t} &\sim C^{+}(0, 1)
\end{align}
where $C^{+}(0, \gamma)$ is the standard half-Cauchy distribution with support on the positive real numbers, and scale $\gamma$.%
\footnote{
  This implies that $p(\lambda^{2})$ is distributed inverse-beta, $IB(a, b)$ where $a = b = \frac{1}{2}$ \parencite[4]{PolsonScott2010}. 
}

The horseshoe prior has two features that make it useful as a shrinkage prior that is robust to structural breaks.
It has flat Cauchy-like tails and an infinitely tall spike.
The flat tails mean that the structural breaks are not shrunk \textit{a posteriori} and the spike around zero aggressively shrinks non-structural breaks.
Figure \ref{fig:horseshoe} plots the density of the horseshoe prior distribution against the normal, Cauchy, and Laplacian (Baysian LASSO) distributions.%
\footnote{
  Although the horseshoe prior distribution's density does not have analytic form, \textcite{CarvalhoPolsonScott2010}, Theorem 1, provides tight bounds on it.
  The density has the same behavior as $\log (1 + \frac{2}{\alpha_{t}^{2}})$.
}

\begin{figure}
  \centering
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics{plots/horseshoe1.pdf}
  \end{subfigure}%
  \begin{subfigure}[b]{0.5\textwidth}
    \includegraphics{plots/horseshoe2.pdf}
  \end{subfigure}
  \caption{The densities of the horseshoe prior, normal, Cauchy, and Laplacian distribution.}
  \label{fig:horseshoe}
\end{figure}

I assume the following non-informative hyper-priors on $\sigma^{2}$ and $\tau$ as in Carvalho et al. (2010, 2009),
\begin{align}
  \label{eq:9}
  p(\sigma^{2}) & \frac{1}{\sigma^{2}}  \\
  \label{eq:11}
  \tau &\sim C^{+}(0, \sigma) \text{.}
\end{align}

\subsection{Identifying Structural Breaks}
\label{sec:ident-struct-breaks}

One of the advantages of the horseshoe prior distribution over other shrinkage distributions is that there is a simple thresholding rule that can be used to identify signals, which in this application are structural breaks.

For the discrete mixture model in equation \eqref{eq:1}, the posterior mean is $E(\eta_{t} | y, \alpha_{t-1}) = p_{t} E(g(\eta_{t}))$, where $p_{t}$ is the posterior inclusion probability of $\eta_{t}$.
For a sufficiently heavy tailed $g$, that posterior mean is approximately $p_{i} e_{i}$.
Note that not only do $p_{t}$ determine whether $\eta_{t}$ should be classified as a signal or noise, they also determine how much $e_{t}$ should be shrunk to 0 when estimating $\eta_{t}$.

A quantity similar to $p_{t}$ can be defined for scale-normal mixture distributions.
Let $\kappa_{t}$ be a shrinkage parameter, defined as
\begin{equation}
  \label{eq:3}
  \kappa_{t} = \frac{\sigma^{2}}{\sigma^{2} + \lambda^{2}_{t} \tau^{2}} \text{.}
\end{equation}
and let $\Delta \alpha_{t} = \alpha_{t} - \alpha_{t - 1}$ and $e_{t} = y_{t} - \alpha_{t - 1}$, 
\begin{equation}
  \label{eq:10}
  E(\eta_{t} | \alpha_{t - 1}, \sigma, \tau, \lambda_{t}) = e_{t}
  \left(
    1 - \frac{\sigma^{2}}{\sigma^{2} + \lambda^{2}_{t} \tau^{2}}
  \right)  = 1 - E(\kappa_{i} | .) e_{i}
\end{equation}
When $\kappa_{t} \approx 0$, the change in $\alpha$ is approximately $y_{t} - \alpha_{t-1}$.
When $\kappa_{t} \approx 1$, the change in $\alpha \approx 0$.

Thus the quantity $1 - \hat \kappa_{t}$ behaves similarly to $p_{t}$ in the discrete mixture.
And in the case of the horseshoe prior distribution, $1 - \hat \kappa_{t} \approx p_{t}$ \parencite[474]{CarvalhoPolsonScott2010}.
Thus, Carvallho \textit{et al.} recommend the following  decision rule under a 0-1 decision rule as to whether an observation is a signal,
\begin{equation}
  \label{eq:5}
  \text{$H_{0,t}$ if $\nu_{t} = 1 - E(\kappa_{t}|y_{t}, \nu_{t-1} \lambda_{t}, \tau, \sigma) > \frac{1}{2}$}
\end{equation}
If a strict decision rule is needed to determine whether an obervation is a structural break is needed, equation \eqref{eq:5} can be used.

\section{Examples}
\label{sec:examples}

\subsection{Nile Flow Data}
\label{sec:nile}

The first example of change-point detection is a classic datset in the state-space and change point literature, the Nile river flow data \textcite{Cobb1978}{Balke1993}{DurbinKoopman2012}
The data consist of annual observations of the flow of the Nile river at Ashwan between 1871 and 1970. 
It is well known that there was a level shift in 1899, both due to the construction of a damn at Ashwan and weather changes.

\subsection{CP6 Data}
\label{sec:cp6-data}



\subsection{Coal Mine Data}
\label{sec:coal-mine-data}



\printbibliography{}

\end{document}

%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 

%  LocalWords:  Carvallho
