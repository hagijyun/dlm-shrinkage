\documentclass{article}

\usepackage{amsmath}
\usepackage{longtable}
\usepackage[backend=biber]{biblatex}

\bibliography{default}

\begin{document}

\section{Model Terminology}


These models are called either state space models (econometrics) or
dynamic (linear) models (statistics / Bayesian).

The following defines a \emph{state space model} 
\begin{equation*}
  \begin{aligned}[t]
    y_t = f(\theta_t \vert  b_{t}, F_t, \nu_t) \\
    \theta_t = f(\theta_{t-1} \vert g_{t},  G_t, \nu_t)
  \end{aligned}
\end{equation*}

If $\theta_t$ is continuous then it is a \emph{continuous state space
model}, if $\theta_t$ is discrete then it is a \emph{discrete state
space model}.

If those equations can be written as 
\begin{equation*}
\begin{aligned}[t]
y_t = b_{t} + F_t \theta_t + \nu_t \\
\theta_t = g_{t} + G_t \theta_{t-1} + \omega_t
\end{aligned}
\end{equation*}
then the model is a \emph{Dynamic Linear Model (DLM)} (linear SSM),
otherwise it is a non-linear dynamic model. If $\nu_t$ and $\omega_t$
are normal distributions, then it is \emph{Guassian} or \emph{Normal
Dynamic Linear Model} (GDLM or NDLM). If $\nu_t$ or $\omega_t$ are

Another class is \emph{Dynamic Generalized Linear Model}.

\section{West and Harrison}

\begin{equation*}
\begin{aligned}[t]
Y_t &= b_{t} + F'_t \theta_{t-1} + \nu_t & \nu_t & \sim N(0, V_t) \\
\theta_t &= g_{t} + G_t \theta_{t-1} + \omega_t & \omega_t & \sim N(0, W_t) \\
\theta_0 &\sim N(m_0, C_0)
\end{aligned}
\end{equation*}
with dimensions $r$ (number of variables), $n$ (number of states),

Matrices,
\begin{longtable}[c]{@{}ll@{}}
\hline\noalign{\medskip}
matrix & dimensions
\\\noalign{\medskip}
\hline\noalign{\medskip}
$F_t$ & $n \times r$
\\\noalign{\medskip}
$G_t$ & $n \times n$
\\\noalign{\medskip}
$V_t$ & $r \times r$
\\\noalign{\medskip}
$W_t$ & $n \times n$
\\\noalign{\medskip}
$C_0$ & $n \times n$
\\\noalign{\medskip}
\hline
\end{longtable}
and vectors,
\begin{longtable}[c]{@{}ll@{}}
\hline\noalign{\medskip}
vector & dimensions
\\\noalign{\medskip}
\hline\noalign{\medskip}
$Y_t$ & $r \times 1$
\\\noalign{\medskip}
$\theta_t$ & $n \times 1$
\\\noalign{\medskip}
$\nu_t$ & $r \times 1$
\\\noalign{\medskip}
$\omega_t$ & $n \times 1$
\\\noalign{\medskip}
$m_0$ & $n \times 1$
\\\noalign{\medskip}
\hline
\end{longtable}

\section{Durbin and Koopmans}

\begin{equation*}
\begin{aligned}[t]
  y_t &= Z_t \alpha_{t} + \varepsilon_t & \varepsilon_t & \sim N(0, H_t) \\
  \alpha_{t+1} &= T_t \alpha_{t} + R_t \eta_t & \eta_t & \sim N(0, Q_t) \\
  \alpha_1 &\sim N(a_1, P_1)
\end{aligned}
\end{equation*}
with dimensions $r$ (number of disturbances), $p$ (number of variables)
and $m$ (number of states).

Matrices,
\begin{longtable}[c]{@{}ll@{}}
\hline\noalign{\medskip}
matrix & dimensions
\\\noalign{\medskip}
\hline\noalign{\medskip}
$Z_t$ & $p \times m$
\\\noalign{\medskip}
$T_t$ & $m \times m$
\\\noalign{\medskip}
$H_t$ & $p \times p$
\\\noalign{\medskip}
$Q_t$ & $r \times r$
\\\noalign{\medskip}
$R_t$ & $m \times r$
\\\noalign{\medskip}
$P_1$ & $m \times m$
\\\noalign{\medskip}
\hline
\end{longtable}
and vectors,
\begin{longtable}[c]{@{}ll@{}}
\hline\noalign{\medskip}
vector & dimensions
\\\noalign{\medskip}
\hline\noalign{\medskip}
$y_t$ & $p \times 1$
\\\noalign{\medskip}
$\alpha_t$ & $m \times 1$
\\\noalign{\medskip}
$\varepsilon_t$ & $p \times 1$
\\\noalign{\medskip}
$\eta_t$ & $r \times 1$
\\\noalign{\medskip}
$a_1$ & $m \times 1$
\\\noalign{\medskip}
\hline
\end{longtable}

This is initialized with the filtered states.

\subsection{General Case}
\label{sec:general-case}

\subsection{Filter}

See \textcite[Chapter 2.7, p. 53]{PetrisPetroneEtAl2009}, with the filter adjusted for intercepts in the observation and system equations.

Let $\theta_{t-1} | y_{1:t-1} \sim N(m_{t-1}, C_{t-1})$.

One step ahead predictive distribution of $\theta_{t-1}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$
\begin{align*}
  a_{t} = E(\theta_{t} | y_{1:t-1}) = g_{t} + G_{t} m_{t-1} \\
  R_{t} = Var(\theta_{t} | y_{1:t-1}) = G_{t} C_{t-1} G'_{t} + W_{t}
\end{align*}

One step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$
\begin{align*}
  f_{t} = E(Y_{t} | y_{1:t-1}) = b_{t} + F_{t} a_{t} \\
  Q_{t} = Var(Y_{t} | y_{1:t-1}) = F_{t} R_{t} F'_{t} + V_{t}
\end{align*}

One step ahead predictive distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
\begin{align*}
  m_{t} = E(\theta_{t} | y_{1:t}) &= a_{t} + R_{t} F'_{t} Q_{t}^{-1} e_{t} \\
  C_{t} = Var(\theta_{t} | y_{1:t}) &= R_{t} - R_{t} F'_{t} Q_{t}^{-1} F_{t} R_{t}
\end{align*}
where $e_{t} = Y_{t} - f_{t}$. The Kalman gain (adaptation coefficient) is defiend as $K_{t} = R_{t} F'_{t} Q_{t}^{-1}$.
\begin{center}
  \begin{tabular}[]{ll}
    \hline
    variable & dim \\
    \hline
    $a_{t}$ & $n$ \tabularnewline
    $R_{t}$ & $n, n$ \tabularnewline
    $f_{t}$ & $r$ \tabularnewline
    $Q_{t}$ & $r, r$ \tabularnewline
    $m_{t}$ & $n$ \tabularnewline
    $C_{t}$ & $n, n$ \tabularnewline
    $e_{t}$ & $r$ \tabularnewline
    $K_{t}$ & $n, r$ 
  \end{tabular}
\end{center}
If all values in $t$ are missing, replace the filter step with,
\begin{align*}
  m_{t} = a_{t} \\
  C_{t} = R_{t} 
\end{align*}
If some are missing (let $r > r_{t} > 0$ be observed), let $M$ be a $r_{t} \times r$ selection matrix and define
\begin{align*}
  y^{*}_{t} = M_{t} y_{t} \\
  F^{*}_{t} = M_{t} F_{t} \\
  V^{*}_{t} = M_{t} V_{t} M_{t}'
\end{align*}

\subsection{Likelihood}
\label{sec:likelihood}

If no missing values, the log likelihood is:
\begin{equation*}
  L(y_{1:T}) =  - \frac{n T}{2} \log (2 \pi) - \frac{1}{2} \sum_{t=1}^{T}
  \left(
    \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
\end{equation*}
If missing values. Let $n_{t}$ be the number of non-missing values in each time period.
\begin{equation*}
  L(y_{1:T}) = 
  -\frac{1}{2} \sum_{t=1}^{T} 
  I(n_{t} > 0) 
  \left(
    n_{t} \log (2 \pi)
    + \log | Q_{t} | + e_{t}' Q_{t}^{-1} e_{t}
  \right)
\end{equation*}

See \textcite[Chapter 5, p. 57]{KoopmanShephardDoornik2008}.

\subsection{Discounting}
\label{sec:smoothing-1}

See \textcite[Chapter 6.3]{WestHarrison1997}

Let $\Delta$ be covariance matrix, with the entries in the diagonal between 0 and 1.

\begin{itemize}
\item Component discounting (West and Harrison)
  \begin{equation*}
    W_{t} = \frac{1 - \delta}{\delta} G_{t} C_{t - 1} G_{t}'
  \end{equation*}
  Let $R_{t} = \Delta^{-1} P_{t}$ and $R_{t} = P_{t} + W_{t}$ where $P_{t} = G_{t} C_{t-1} G_{t}'$,
  \begin{equation*}
    W_{t} = (\Delta^{-1} - I) P_{t} 
  \end{equation*}
\item Let $R_{t} = \Delta G_{t} C_{t-1} G'_{t} \Delta$ (p. 202)
\item Let $R_{t} = G_{t} \Delta C_{t-1} \Delta G'_{t}$ (p. 202)
\end{itemize}

\subsection{Smoothing}
\label{sec:smoothing}

See \textcite[Prop 2.4, p. 61]{PetrisPetroneEtAl2009}

If $\theta_{t+1} | y_{1:T} \sim N(s_{t+1}, S_{t+1})$ then $\theta_{t} | y_{1:T} \sim N(s_{t}, S_{t})$ where
\begin{align*}
  s_{t} = E(\theta_{t} | y_{1:T}) = m_{t} + C_{t} G'_{t+1} R_{t+1}^{-1}(s_{t+1} - a_{t+1}) \\
  S_{t} = Var(\theta_{t} | y_{1:T}) = C_{t} - C_{t} G'_{t+1} R^{-1}_{t+1} (R_{t+1} - S_{t+1}) R^{-1}_{t+1} G_{t+1} C_{t}
\end{align*}

\subsection{Backward Sample}
\label{sec:backward-sample}

\textcite[Chapter 4.4.1, p. 161]{PetrisPetroneEtAl2009}

Supposing that $m_{1:T}$, $C_{1:T}$, $a_{1:T}$ and $R_{1:T}$ have been calculated by the filter,%
\footnote{No additional adjustment for intercepts required because it operates on $a_{t}$}

To draw $\theta_{1:T} | y_{1:T}$,

\begin{enumerate}
\item From the Kalman filter, $\theta_{T} | y_{1:T} \sim N(m_{T}, C_{T})$
\item For $t = (T-1):0$, $\theta_{t} | y_{1:T} \sim N(h_{t}, H_{t})$ where
  \begin{align*}
    h_{t} = m_{t} + C_{t} G'_{t + 1} R_{t+1}^{-1}(\theta_{t-1} - a_{t+1}) \\
    H_{t} = C_{t} - C_{t + 1} G'_{t} R_{t+1}^{-1} G_{t+1} C_{t}
  \end{align*}
\end{enumerate}

\subsection{Univariate Local Level Model}
\label{sec:local-level-model}

For the univariate local level model, some of the steps can be simplified with $F_{t} = G_{t} = 1$,
and $g_{t} = b_{t} = 0$.

One step ahead predictive distribution of $\theta_{t-1}$ given $y_{1:t-1}$ is $N(a_{t}, R_{t})$
\begin{align*}
  a_{t} = E(\theta_{t} | y_{1:t-1}) = m_{t-1} \\
  R_{t} = Var(\theta_{t} | y_{1:t-1}) = C_{t-1} + W_{t}
\end{align*}

One step ahead predictive distribution of $Y_{t}$ given $y_{1:t-1}$ is $N(f_{t}, Q_{t})$
\begin{align*}
  f_{t} = E(Y_{t} | y_{1:t-1}) = a_{t} = m_{t-1} \\
  Q_{t} = Var(Y_{t} | y_{1:t-1}) = R_{t} + V_{t} = C_{t-1} + W_{t} + V_{t}
\end{align*}

One step ahead predictive distribution of $\theta_{t}$ given $y_{1:t}$ is $N(m_{t}, C_{t})$
\begin{align*}
  f_{t} = E(\theta_{t} | y_{1:t}) = a_{t} + R_{t} Q_{t}^{-1} e_{t} = m_{t-1} + \frac{C_{t-1} + W_{t}}{C_{t-1} + W_{t} + V_{t}} e_{t} \\
  Q_{t} = Var(\theta_{t} | y_{1:t}) = R_{t} - \frac{(C_{t-1} + W_{t})^{2}}{C_{t-1} + W_{t} + V_{t}}
\end{align*}
where $e_{t} = Y_{t} - f_{t}$. The Kalman gain is defiend as $K_{t} = R_{t} F'_{t} Q_{t}^{-1}$.

\section{Notes}

\begin{itemize}
\item Use discounting
  \begin{itemize}
  \item Use a beta rectangular distribution
  \item Beta distribution with priors over $\alpha$  and $\beta$
  \end{itemize}
\end{itemize}




\end{document}


%%% Local Variables: 
%%% mode: latex
%%% TeX-master: t
%%% End: 
